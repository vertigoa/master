{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Performance Assessment: D208 Predictive Modeling Task 1 - Multiple Linear Regression.\n",
    "\n",
    "## Michael Hindes\n",
    "Department of Information Technology, Western Governors University\n",
    "<br>D208: Predictive Modeling\n",
    "<br>Professor Dr. Straw\n",
    "<br>March 21, 2024\n",
    "\n",
    "\n",
    "# Part I: Research Question\n",
    "## Describe the purpose of this data analysis by doing the following::\n",
    "\n",
    "### **A1. Research Question:**\n",
    "**A1. Research Question:\n",
    "\"What factors contribute to the length of a patient's initial hospital stay?\"**\n",
    "\n",
    "This question aims to identify key variables within the dataset that influence `Initial_days`; The number of days the patient stayed in the hospital during the initial visit to the hospital. \n",
    "\n",
    "### **A2. Define the goals of the data analysis.**\n",
    "\n",
    "The project sets out to explore the relationship between a response and predictor variables by exploring raw medical data and developing a multiple linear regression model. The research question focuses on identifying any potential factors that affect the length of a patient's hospital stay by exploring factors such as demographic details, medical history, financial factors, and services received. Python and associated libraries are used for analysis, and that supported by visual aids for clarity. Data cleaning and wrangling is emphasized to ensure accuracy and reliability.The Python code for analysis, data cleaning, and preparation will be shared. The culmination of this project involves creating, evaluating and reducing a multiple linear regression model, discussing its significance both statistically and practically, highlighting limitations, and suggesting actionable steps for stakeholders and future analysts based on the findings. Length of stay is a critical metric in healthcare, as it can impact resource allocation, patient satisfaction, and overall hospital efficiency. By identifying the factors that contribute to a patient's hospital stay, healthcare providers can optimize their services, improve patient outcomes, and enhance the overall quality of care.\n",
    "\n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part II: Method Justification\n",
    "\n",
    "## B. Describe multiple linear regression methods by doing the following:\n",
    "\n",
    "### **B1. Summarize four assumptions of a multiple linear regression model:**\n",
    "\n",
    "In the research on the assumption of multiple linear regression, I consistently found five key assumptions in some places and four in others in different combinations. They all appear critical to the validity of a model. As such I will list five assumptions below. (Statology, n.d.)\n",
    "\n",
    "-   **Linearity** asserts that there is a straight-line relationship between each predictor (independent variable) and the response (dependent variable). In other words, a straight line can best represent the average change in the dependent variable for a unit change in the independent variable, holding all other independent variables constant. This can be assessed through visualizations.\n",
    "\n",
    "-   **Little to no Multicollinearity** the data points in the dataset do not influence each other. Each observation's response is determined by its predictor values, and should be free from the influence of  other independent variables. Multicollinearity is often checked with the Variance Inflation Factor (VIF) or correlation matrix.\n",
    "\n",
    "-   **Independence of Observations** assumes that the observations in the dataset are independent of each other. This means that the value of one observation should not be influenced by the value of another observation. \n",
    "\n",
    "-   **Homoscedasticity** refers to the requirement that the error terms (differences between observed and predicted values) maintain a constance variance across all points. This constant variance ensures that the model's accuracy does not depend on the value of the predictors. Homoscedasticity is often checked with a residuals plots to look for patterns where there should be none, and can be caused by a variety of factors.\n",
    "\n",
    "-   **Normality of Errors** states that the residuals (errors) in the model are normally distributed around a mean of zero. This can be checked with a histogram or Q-Q plot of the residuals. If the residuals are not normally distributed, the model may not be accurate.\n",
    "\n",
    "### **B2. Describe two benefits of using Python for data analysis:**\n",
    "\n",
    "- **Rich Libraries:** While R was specifically designed with statistics and data analysis in mind, Python was chosen for its suite of libraries that facilitate every phase of the data analysis process. Libraries such as Pandas for data manipulation, NumPy for numerical computations, and Matplotlib along with Seaborn for visualizations. Statsmodels and Scikit-learn offers a platforms for applying regression and machine learning algorithms, streamlining the development of predictive models. These libraries help with a range of data analysis tasks.\n",
    "\n",
    "- **Versatility** Python's syntax is known for its intuitiveness and readability, and wide ranging application, making it a favorite for many, from data science to web development. This versatility extends beyond data analysis to other applications such as web development, automation, and deep learning. For instance, an analyst can easily switch from analyzing data to deploying a machine-learning model as a web application within the same programming environment. This flexibility is a significant advantage for working across multiple domains. (Western Governors University, n.d.)\n",
    "\n",
    "### **B3. Explain why multiple linear regression is an appropriate technique for analyzing the research question summarized in part I:**\n",
    "\n",
    "Multiple linear regression is suited well for addressing the research question at hand. Is needed because unlike simple linear regression, there can be multiple variables which is the case here. MLR is a statistical technique that uses several explanatory or predictor (independent) variables to predict the outcomeâ€‹ of a response or target (dependent) variable, in this case `Initial_days`, the length of time a patients initial stay in the hospital was. This analytical technique is adept at not only identifying but also quantifying the strength and nature of the relationships between `Initial_days` and various predictors. It accounts for multiple factors simultaneously, which can provide a more nuanced insights into their combined effects on the length of a hospital stay. This is necessary for creating a predictive model that can inform decision-making processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Data Preparation\n",
    "\n",
    "## C. Summarize the data preparation process for multiple linear regression analysis by doing the following:\n",
    "\n",
    "### **C1. Describe your data cleaning goals and the steps used to clean the data to achieve the goals that align with your research question including your annotated code.**\n",
    "\n",
    "The cleaning process starts by reading the data into a pandas DataFrame and performing an initial examination to gain a preliminary understanding of its structure and content. This involves checking data types, identifying duplicate rows, and detecting missing values. Outliers are important to detect and be aware of, particularly when creating predictive regression models. In the context of medical data, outliers can often be the very things that are of interest, such as patients with very high cholesterol levels or very low blood pressure. These values are not necessarily errors but rather important indicators of health conditions. Therefore, outliers will be noted but not necessarily treated unless they are obvious data entry errors or if they hinder the model.\n",
    "\n",
    "Unique values will be scrutinized to understand the diversity of information within the dataset, dropping unnecessary columns that are not relevant to the research question or predictive model, and converting categorical variables into numerical formats. Some demographic and identifier data, which represents static information about patients and cannot be altered by the hospital, will be excluded from the analysis. Missing data will be identified and addressed, ensuring its proper mitigation, and any duplicate records will be eliminated. Renaming of certain variables for a more descriptive understanding. Rounding data to a reasonable number of decimal places can improve readability and reduce computational complexity. Data visualizations such as scatter plots, histograms, and box plots will be used to understand the relationships between variables and identify patterns in the data distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following requirements from `Part C` of the performance assessment will be demonstrated in the multiple cells below.\n",
    "-   **C2.  Describe the dependent variable and all independent variables using summary statistics that are required to answer the research question.**\n",
    "\n",
    "-   **C3.  Generate univariate and bivariate visualizations of the distributions of the dependent and independent variables.**\n",
    "\n",
    "-   **C4.  Describe your data transformation goals that align with your research question**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(The Python code used in the project was heavily informed and sometimes directly puulled from the documentation listed in the `Software` section of the References section concluding this project )*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and libraries\n",
    "%pip install scikit-learn\n",
    "%pip install Jinja2\n",
    "%matplotlib inline\n",
    "%pip install statsmodels\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data variable description and data types with examples.\n",
    "from IPython.display import Image\n",
    "Image(filename='variable_description_208.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data and read it into a dataframe, setting the first column `CaseOrder` as the index for consistency.\n",
    "df_medical = pd.read_csv('D208_templates/medical_clean.csv', index_col=0)\n",
    "\n",
    "# Display the first five rows of the data\n",
    "df_medical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the last 5 rows of the dataframe\n",
    "df_medical.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the DataFrame information\n",
    "df_medical.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows. \n",
    "print(df_medical.duplicated().value_counts())\n",
    "print('Total Duplicated Rows: ', df_medical.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "df_medical.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns Item 1 to Item 8 to the appropriate column names. The 'S_' modifier is used to indicate the column is a survey item.\n",
    "new_col_names={\n",
    "    'Item1':'S_T_Admission',\n",
    "    'Item2':'S_T_Treatment', \n",
    "    'Item3':'S_T_Visits', \n",
    "    'Item4':'S_Reliability', 'Item5':'S_Options', \n",
    "    'Item6':'S_Hours_Treatment', \n",
    "    'Item7':'S_Staff', \n",
    "    'Item8':'S_Active_Listening'}\n",
    "df_medical.rename(columns=new_col_names, inplace=True)\n",
    "df_medical.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the data types and unique values count into a DataFrame easy reference and comparison\n",
    "data_types = df_medical.dtypes\n",
    "unique_values = df_medical.nunique()\n",
    "comparison_df = pd.DataFrame({'Data Type': data_types, 'Unique Values': unique_values})\n",
    "comparison_df.sort_values(by='Unique Values', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cardinality and Data Type Summary of Variables\n",
    "\n",
    "## Numerical Variables\n",
    "- `Income`: 9993 unique values (float64)\n",
    "- `VitD_levels`: 9976 unique values (float64)\n",
    "- `Initial_days`: 9997 unique values (float64)\n",
    "- `TotalCharge`: 9997 unique values (float64)\n",
    "- `Additional_charges`: 9418 unique values (float64)\n",
    "- `Population`: 5951 unique values (int64)\n",
    "- `Children`: 11 unique values (int64)\n",
    "- `Age`: 72 unique values (int64)\n",
    "- `Doc_visits`: 9 unique values (int64)\n",
    "- `Full_meals_eaten`: 8 unique values (int64)\n",
    "- `vitD_supp`: 6 unique values (int64)\n",
    "- `Lat`: 8588 unique values (float64)\n",
    "- `Lng`: 8725 unique values (float64)\n",
    "\n",
    "## Ordinal Variables (Categorical)\n",
    "- `S_T_Admission`: 8 unique values (int64)\n",
    "- `S_T_Treatment`: 7 unique values (int64)\n",
    "- `S_T_Visits`: 8 unique values (int64)\n",
    "- `S_Reliability`: 7 unique values (int64)\n",
    "- `S_Options`: 7 unique values (int64)\n",
    "- `S_Hours_Treatment`: 7 unique values (int64)\n",
    "- `S_Staff`: 7 unique values (int64)\n",
    "- `S_Active_Listening`: 7 unique values (int64)\n",
    "\n",
    "## Nominal Variables (Categorical)\n",
    "- `Customer_id`: 10000 unique values (object)\n",
    "- `Interaction`: 10000 unique values (object)\n",
    "- `UID`: 10000 unique values (object)\n",
    "- `City`: 6072 unique values (object)\n",
    "- `State`: 52 unique values (object)\n",
    "- `County`: 1607 unique values (object)\n",
    "- `Zip`: 8612 unique values (int64)\n",
    "- `Area`: 3 unique values (object)\n",
    "- `TimeZone`: 26 unique values (object)\n",
    "- `Job`: 639 unique values (object)\n",
    "- `Marital`: 5 unique values (object)\n",
    "- `Gender`: 3 unique values (object)\n",
    "- `ReAdmis`: 2 unique values (object)\n",
    "- `Soft_drink`: 2 unique values (object)\n",
    "- `Initial_admin`: 3 unique values (object)\n",
    "- `HighBlood`: 2 unique values (object)\n",
    "- `Stroke`: 2 unique values (object)\n",
    "- `Complication_risk`: 3 unique values (object)\n",
    "- `Overweight`: 2 unique values (object)\n",
    "- `Arthritis`: 2 unique values (object)\n",
    "- `Diabetes`: 2 unique values (object)\n",
    "- `Hyperlipidemia`: 2 unique values (object)\n",
    "- `BackPain`: 2 unique values (object)\n",
    "- `Anxiety`: 2 unique values (object)\n",
    "- `Allergic_rhinitis`: 2 unique values (object)\n",
    "- `Reflux_esophagitis`: 2 unique values (object)\n",
    "- `Asthma`: 2 unique values (object)\n",
    "- `Services`: 4 unique values (object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given the nature of the data, there are several variables that will be excluded from the analysis. Here is a brief summary of the variables that will be excluded and the rationale for their exclusion:**\n",
    "\n",
    "### Current Strategy Overview:\n",
    "1. **Broad Inclusion**: *Cast a wide net* (Middleton, 2024) Start with a wide array of variables to capture potential influences on `Initial_days`, informed by domain knowledge and based on the reccomendation of the instructors of this course. \n",
    "2. **Build Initial Model**: Use this dataset to identify significant variables.\n",
    "3. **Analyze & Refine**: Eliminate non-contributing or highly correlated variables based on initial model insights.\n",
    "4. **Develop Reduced Model**: Focus on key variables for a streamlined, effective model.\n",
    "\n",
    "### Variables Eliminated:\n",
    "*Note: I am a former health care professional who has worked in several hospitals and have had extensive hospital stays as a patient. While I am not an expert on this particular data, I do have some domain knowledge and this domain knowledge informs my decision making here.*\n",
    "- **TotalCharge & Additional Charges**: Possible high correlation and generally a result of `Initial_days` not a cause of. Patients and staff often unaware of these charges until after the fact.\n",
    "- **Latitude & Longitude**: Limited interpretive value and adds to model complexity.\n",
    "- **Identifiers (Customer_id, Interaction, UID)**: High uniqueness; ethical concerns.\n",
    "- **Geographic (City, State, County, Zip, Population)**: Overly detailed, increasing model complexity, not short/medium term actionable.\n",
    "- **TimeZone**: Relevance to hospital stay length is questionable, increases complexity.\n",
    "- **Full_meals_eaten**: Restrictive and targeted diets and meals are so common and depends on patient and services that without context ths variable is not useful.\n",
    "- **Job**: Subjective and variable in interpretation. Better suited for targeted occupational study.\n",
    "- **Services**: All very common in diagnostic phase and itself dependent on too many unknown factors, and not likely to be significant predictors. Could add confusion. \n",
    "- **Soft_drink**: Poorly defined as soft drink can mean anything from uncaffinated carbonated water to a caffinated sugary soda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create reduced dataframe with only the columns  for the analysis\n",
    "colms_to_drop = ['TotalCharge', 'Services', 'Soft_drink', 'Additional_charges', 'Lat', 'Full_meals_eaten', 'Lng', 'Customer_id', 'Interaction', 'UID', 'City', 'State', 'County', 'Zip', 'TimeZone', 'Job', 'Population']\n",
    "\n",
    "df_reduced = df_medical.drop(colms_to_drop, axis=1)\n",
    "\n",
    "# display the dataframe in full\n",
    "pd.set_option('display.max_columns', None)\n",
    "df_reduced.head().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Stats For numeric variables\n",
    "selected_columns = df_reduced[['Age', 'Income', 'Children', 'VitD_levels', 'Doc_visits', 'vitD_supp', 'Initial_days']].copy()\n",
    "selected_columns.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Takeaways:\n",
    "\n",
    "- **Age**: Averages 53 years, ranging from 18 to 89, with a diverse age profile.\n",
    "- **Income**: Averages $40,490, with wide variation (154 to 207249), indicating economic diversity.\n",
    "- **Children**: Averages 2 children with a similar median, with a range of 0 to 10.\n",
    "- **VitD_levels**: Averages 17.96, mostly within a narrow range (9.81 to 26.39), suggesting more consistent levels across patients.\n",
    "- **Doc_visits**: Averages 5 visits, indicating a similar frequency of medical consultations.\n",
    "- **vitD_supp**: Averages less than 0.5 supplements, with low intake common among patients.\n",
    "\n",
    "- **Categorical** nominal and ordinal variables are not included here and will include a separate summary of proportions along wit univariate and bivariate visualizations.\n",
    "- **Initial_days**: Our dependent (target) variable will be fully summarize and visualized below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rounding Justification. \n",
    "-    Rounding 'Initial_days' from 8 decimal places to 2significantly reduces the number of unique values, which can simplify analyses and visualizations by reducing the granularity of the data. Precision beyond 2 decimal places does not add meaningful information for the analysis. In many practical scenarios, especially related to days, a precision of 2 decimal places is sufficient to capture relevant variations without unnecessarily complicating the dataset.  In healthcare data, for instance, it's unlikely that fractions of a day to eight decimal places would impact decisions or care outcomes.\n",
    "\n",
    "- Similarly, rounding 'Income' to whole numbers, and 'VitD_levels' to 2 decimal places seems appropriate in this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round 'Initial_days' and 'VitD_levels' to 2 decimal places\n",
    "df_reduced = df_reduced.round({'VitD_levels': 2})\n",
    "df_reduced = df_reduced.round({'Initial_days': 2})\n",
    "\n",
    "# round 'Income' to 0 decimal places by converting to integer\n",
    "df_reduced = df_reduced.astype({'Income': 'int64'})\n",
    "\n",
    "# fisplay the dataframe with the rounded values\n",
    "df_reduced[['Initial_days', 'VitD_levels', 'Income']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv and to save results so far and to reduce memory consumption.\n",
    "df_reduced.to_csv('df_reduced.csv', index='CaseOrder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('df_reduced.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C3.  Visualizations \n",
    "\n",
    "Below are Univariate and Bivariate Visualizations for independent variables showing their relationship with the dependent variable `Initial_days`. Seaborn and Matplotlib will be used to create visualizations and the choice of graph will depend on the nature of the variable being visualized. (Python Graph Gallery. n.d), (Eyre, 2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univaraite Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot for 'Initial_days'\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(x=df['Initial_days'])\n",
    "plt.title('Boxplot for Initial_days')\n",
    "plt.show()\n",
    "\n",
    "# Histogram for 'Initial_days'\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.histplot(data=df, x='Initial_days', kde=True, bins=50)\n",
    "plt.title('Histogram for Initial_days')\n",
    "plt.show()\n",
    "\n",
    "df['Initial_days'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Boxplot Observations**: The median appears to be above the mid-30s, suggesting that roughly half of the patients have shorter initial stays and the other half have longer. There are no visible outliers, indicating no extreme values or anomalies that fall outside the typical range. The interfertile range shows that the middle 50% of the data spans a rather large range, suggesting a concentration of data within this segment.\n",
    "\n",
    "- **Histogram Observations**: The distribution is bimodal, with two peaks: one just under a few days and another around 70 days. This suggests there are two groups of patients with different typical hospital stay lengths. The histogram indicates that shorter initial stays are more common than longer stays, with a significant drop-off in frequency as the number of days increases towards the middle values. The spread between the two modes shows that there is variability in the data, not concentrated around a single central value. Understanding the reasons behind this bimodal distribution may require further investigation into the factors affecting hospital stay lengths. This distribution is important to kee in mind when interpreting the results of the regression analysis, as it may influence the model's predictive accuracy and the significance of the predictors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**: Statistical measures for `Initial_days` across all patients in the dataset, including:\n",
    "\n",
    "- **Count**: 10,000 observations. This represents the number of patients included in the analysis.\n",
    "- **Mean**: Approximately 34 days. On average, patients spend a little over a month in the hospital.\n",
    "- **Standard Deviation**: About 26 days. This indicates a wide variation in the length of hospital stays among patients; while some patients have short stays, others have significantly longer stays.\n",
    "- **Minimum**: Just over 1 day. This shows that some patients are discharged almost immediately after admission.\n",
    "- **25% (First Quartile)**: About 8 days or less. A quarter of the patients have hospital stays just over a week.\n",
    "- **Median (50%)**: Approximately 36 days. This is very close to the mean. However, the slight difference between the mean and median indicates a slight skew in the data.\n",
    "- **75% (Third Quartile)**: About 61 days or less. Most patients are discharged within two months.\n",
    "- **Maximum**: Nearly 72 days. Indicates that some patients have extended hospital stays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subplots for the histplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
    "\n",
    "# distribution/count of children\n",
    "sns.histplot(data=df, x='Children', ax=axes[0], kde=True)\n",
    "axes[0].set_title('Distribution/Count of Children')\n",
    "\n",
    "# distribution of ages\n",
    "sns.histplot(data=df, x='Age', ax=axes[1], kde=True)\n",
    "axes[1].set_title('Distribution of Ages')\n",
    "\n",
    "# distribution of income\n",
    "sns.histplot(data=df, x='Income', ax=axes[2], kde=True)\n",
    "axes[2].set_title('Distribution of Income')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# summary statistics for the variables\n",
    "df[['Children', 'Age', 'Income']].describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subplots for the boxplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
    "\n",
    "# boxplot of children\n",
    "sns.boxplot(data=df, x='Children', ax=axes[0])\n",
    "axes[0].set_title('Boxplot of Children')\n",
    "\n",
    "# boxplot of ages\n",
    "sns.boxplot(data=df, x='Age', ax=axes[1])\n",
    "axes[1].set_title('Boxplot of Ages')\n",
    "\n",
    "# boxplot of income\n",
    "sns.boxplot(data=df, x='Income', ax=axes[2])\n",
    "axes[2].set_title('Boxplot of Income')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The outliers here will be noted as they may impact the regression model, particularly with OLS regression. For now, we will note them and include as is in the initial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subplots for the histplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
    "\n",
    "# distribution/count of VitD_levels\n",
    "sns.histplot(data=df, x='VitD_levels', ax=axes[0], kde=True)\n",
    "axes[0].set_title('Distribution of VitD_levels')\n",
    "\n",
    "# distribution/count of Doc_visits with bigger bins\n",
    "sns.histplot(data=df, x='Doc_visits', ax=axes[1], kde=True)\n",
    "axes[1].set_title('Distribution/Count of Doc_visits')\n",
    "\n",
    "# distribution/count of vitD_supp with bigger bins\n",
    "sns.histplot(data=df, x='vitD_supp', ax=axes[2], kde=True)\n",
    "axes[2].set_title('Distribution/count of vitD_supp')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# descriptive statistics for the variables\n",
    "df[['VitD_levels', 'Doc_visits', 'vitD_supp']].describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `Vitamin D levels` appear normally distributed around a middle value, suggesting that most patients have Vitamin D levels within a standard range, with fewer individuals having very high or very low levels. `Doc_visits` show a pattern with most patientss having 4-6 visits, and the frequency drops for higher numbers of visits. For Vitamin `D supplements`, most patients are not given supplements, which aligns with the distribution of Vitamin D levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2 by 2 subplot grid\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 6))\n",
    "\n",
    "# Area\n",
    "sns.countplot(data=df, x='Area', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Count of Area')\n",
    "\n",
    "# Marital\n",
    "sns.countplot(data=df, x='Marital', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Count of Marital Status')\n",
    "\n",
    "# Gender\n",
    "sns.countplot(data=df, x='Gender', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Count of Gender')\n",
    "\n",
    "# ReAdmis\n",
    "sns.countplot(data=df, x='ReAdmis', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Numer of ReAdmissions')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# create a countplot for initial_admin\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.countplot(data=df, x='Initial_admin')\n",
    "plt.title('Count of Initial_admin')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proportion Summary \n",
    "\n",
    "`Area`\n",
    "- Rural: 33.69%\n",
    "- Urban: 33.03%\n",
    "- Suburban: 33.28%\n",
    "\n",
    "`Gender`\n",
    "- Female: 50.18%\n",
    "- Male: 47.68%\n",
    "- Nonbinary: 2.14%\n",
    "\n",
    "`Marital`\n",
    "- Widowed: 20.45% \n",
    "- Married: 20.23% \n",
    "- Separated: 19.87% \n",
    "- Never Married: 19.84% \n",
    "- Divorced: 19.61%\n",
    "\n",
    "`ReAdmis`\n",
    "- No: 63.31%\n",
    "- Yes: 36.69%\n",
    "\n",
    "`Initial_admin`\n",
    "- Emergency: 51.60%\n",
    "- Elective: 25.04%\n",
    "- Observation: 24.36%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survey items\n",
    "# 2 by 4 subplot grid\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "\n",
    "# S_T_Admission\n",
    "sns.countplot(data=df, x='S_T_Admission', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Count of S_T_Admission')\n",
    "\n",
    "# S_T_Treatment\n",
    "sns.countplot(data=df, x='S_T_Treatment', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Count of S_T_Treatment')\n",
    "\n",
    "# S_T_Visits\n",
    "sns.countplot(data=df, x='S_T_Visits', ax=axes[0, 2])\n",
    "axes[0, 2].set_title('Count of S_T_Visits')\n",
    "\n",
    "# S_Reliability\n",
    "sns.countplot(data=df, x='S_Reliability', ax=axes[0, 3])\n",
    "axes[0, 3].set_title('Count of S_Reliability')\n",
    "\n",
    "# S_Options\n",
    "sns.countplot(data=df, x='S_Options', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Count of S_Options')\n",
    "\n",
    "# S_Hours_Treatment\n",
    "sns.countplot(data=df, x='S_Hours_Treatment', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Count of S_Hours_Treatment')\n",
    "\n",
    "# S_Staff\n",
    "sns.countplot(data=df, x='S_Staff', ax=axes[1, 2])\n",
    "axes[1, 2].set_title('Count of S_Staff')\n",
    "\n",
    "# S_Active_Listening\n",
    "sns.countplot(data=df, x='S_Active_Listening', ax=axes[1, 3])\n",
    "axes[1, 3].set_title('Count of S_Active_Listening')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# value counts for the survey items\n",
    "df[['S_T_Admission', 'S_T_Treatment', 'S_T_Visits', 'S_Reliability', 'S_Options', 'S_Hours_Treatment', 'S_Staff', 'S_Active_Listening']].apply(pd.Series.value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Survey responses across various rating scales appear to be fairly evenly distributed among the different survey items. This uniformity could indicate a degree of correlation among the responses to these items. To explore potential patterns, we will utilize pie charts to visualize the distribution of responses and a correlation matrix to quantitatively assess the relationships between the items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textprops = {\"fontsize\":10} \n",
    "\n",
    "# 2 by 4 subplot grid\n",
    "fig, axes = plt.subplots(4, 2, figsize=(6, 10))\n",
    "\n",
    "# S_T_Admission\n",
    "df['S_T_Admission'].value_counts().plot.pie(ax=axes[0, 0], autopct='%1.1f%%', textprops=textprops)\n",
    "axes[0, 0].set_title('S_T_Admission')\n",
    "\n",
    "# S_T_Treatment\n",
    "df['S_T_Treatment'].value_counts().plot.pie(ax=axes[0, 1], autopct='%1.1f%%', textprops=textprops)\n",
    "axes[0, 1].set_title('S_T_Treatment')\n",
    "\n",
    "# S_T_Visits\n",
    "df['S_T_Visits'].value_counts().plot.pie(ax=axes[1, 0], autopct='%1.1f%%', textprops=textprops)\n",
    "axes[1, 0].set_title('S_T_Visits')\n",
    "\n",
    "# S_Reliability\n",
    "df['S_Reliability'].value_counts().plot.pie(ax=axes[1, 1], autopct='%1.1f%%', textprops=textprops)\n",
    "axes[1, 1].set_title('S_Reliability')\n",
    "\n",
    "# S_Options\n",
    "df['S_Options'].value_counts().plot.pie(ax=axes[2, 0], autopct='%1.1f%%', textprops=textprops)\n",
    "axes[2, 0].set_title('S_Options')\n",
    "\n",
    "# S_Hours_Treatment\n",
    "df['S_Hours_Treatment'].value_counts().plot.pie(ax=axes[2, 1], autopct='%1.1f%%', textprops=textprops)\n",
    "axes[2, 1].set_title('S_Hours_Treatment')\n",
    "\n",
    "# S_Staff\n",
    "df['S_Staff'].value_counts().plot.pie(ax=axes[3, 0], autopct='%1.1f%%', textprops=textprops)\n",
    "axes[3, 0].set_title('S_Staff')\n",
    "\n",
    "# S_Active_Listening\n",
    "df['S_Active_Listening'].value_counts().plot.pie(ax=axes[3, 1], textprops=textprops)\n",
    "axes[3, 1].set_title('S_Active_Listening')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# display descriptive statistics for the survey items\n",
    "df[['S_T_Admission', 'S_T_Treatment', 'S_T_Visits', 'S_Reliability', 'S_Options', 'S_Hours_Treatment', 'S_Staff', 'S_Active_Listening']].describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The pie charts and summary show similar pattern across with some survey responses having almost identical proportions. This could indicate a lack of variability in the responses, which may impact the predictive power of these variables in the regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a correlation matrix\n",
    "#  the columns correlation matrix\n",
    "cols = ['S_T_Admission', 'S_T_Treatment', 'S_T_Visits', 'S_Reliability',\n",
    "        'S_Options', 'S_Hours_Treatment', 'S_Staff', 'S_Active_Listening']\n",
    "\n",
    "corr_matrix = df[cols].corr()\n",
    "\n",
    "# Create a heatmap of the correlation matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The correlation matrix shows that there may indeed be correlation amongst the different survey items. This could introduce multicolinarity into the regression model.\n",
    "\n",
    "- These pairs of items have correlation coefficients very close to zero, suggesting that there is little to no linear relationship between them. When selecting variables for a regression model, these items might be preferred as they are less likely to introduce multicollinearity issues. We will note this during the initial model building phase. And check the VIF scores to confirm.\n",
    "\n",
    "- S_T_Admission and S_Reliability: -0.0046\n",
    "- S_T_Visits and S_Reliability: -0.0063\n",
    "- S_T_Admission and S_Options: -0.0037\n",
    "- S_T_Treatment and S_Options: -0.01\n",
    "- S_T_Visits and S_Options: -0.01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bivariate Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate Graphs with Initial_days\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "#  vitD_levels vs. Initial_days\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.regplot(data=df, x='VitD_levels', y='Initial_days', scatter_kws={'edgecolor':'black'}, line_kws={'color':'orange'})\n",
    "plt.title('VitD_levels vs. Initial_days')\n",
    "\n",
    "#Children vs. Initial_days\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.regplot(data=df, x='Children', y='Initial_days', scatter_kws={'edgecolor':'black'}, line_kws={'color':'orange'})\n",
    "plt.title('Children vs. Initial_days')\n",
    "\n",
    "# Aage vs. Initial_days\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.regplot(data=df, x='Age', y='Initial_days', scatter_kws={'edgecolor':'black'}, line_kws={'color':'orange'})\n",
    "plt.title('Age vs. Initial_days')\n",
    "\n",
    "# income vs. Initial_days\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.regplot(data=df, x='Income', y='Initial_days', scatter_kws={'edgecolor':'black'}, line_kws={'color':'orange'})\n",
    "plt.title('Income vs. Initial_days')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.regplot(data=df, x='Doc_visits', y='Initial_days', scatter_kws={'edgecolor':'black'}, line_kws={'color':'orange'})\n",
    "plt.title('Initial_days vs. Doc_visits')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - `Vitamin D` levels and initial days don't seem to have a clear pattern, with no obvious relationship. When it comes to `children` ther is no distinct trend, suggesting the number of children doesn't linearly affect the length of hospital stay. `Age` shows a spread of data across the age range without a strong trend. For `income`, there's more variability at higher income levels, but there is no clear pattern suggesting a strong relationship. Overall, these plots suggest that individually, these variables do not have a simple linear relationship with the number of initial days spent in the hospital. However, together they might. Income might benifit from a transformation to better understand the relationship. `Doc_visits` suggest that there is no strong, straightforward relationship between the number of doctor visits and the average initial days, as increased doctor visits do not correlate with either a significant increase or decrease in the initial days. The bimodal distrubution of Initial_days may be why one sees distributions grouped above and below the lines.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate Graphs with Initial_days\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Marital\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.boxplot(data=df, x='Marital', y='Initial_days', color='lightgreen')\n",
    "sns.pointplot(data=df, x='Marital', y='Initial_days', color='red', estimator=np.mean, errorbar=None)\n",
    "plt.title('Initial_days vs Marital statuses')\n",
    "\n",
    "# Gender\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.boxplot(data=df, x='Gender', y='Initial_days', color='lightgreen')\n",
    "sns.pointplot(data=df, x='Gender', y='Initial_days', color='red', estimator=np.mean, errorbar=None)\n",
    "plt.title('Initial_days vs Gender categories')\n",
    "\n",
    "# Initial_days for Readmission status\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.boxplot(data=df, x='ReAdmis', y='Initial_days', color='lightgreen')\n",
    "sns.pointplot(data=df, x='ReAdmis', y='Initial_days', color='red', estimator=np.mean, errorbar=None)\n",
    "plt.title('Initial_days for Readmission status')\n",
    "\n",
    "# Initial_days vs Area categories\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.boxplot(data=df, x='Area', y='Initial_days', color='lightgreen')\n",
    "sns.pointplot(data=df, x='Area', y='Initial_days', color='red', estimator=np.mean, errorbar=None)\n",
    "plt.title('Initial_days vs Area')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# boxplot with Initial_admin and Initial_days\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.boxplot(data=df, x='Initial_admin', y='Initial_days', color='lightgreen')\n",
    "sns.pointplot(data=df, x='Initial_admin', y='Initial_days', color='red', estimator=np.mean, errorbar=None)\n",
    "plt.title('Boxplot for Initial_admin and Initial_days')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `Marital` statuses plot shows that seperated and single patients tend to have the highest number of days in the hospital, and that divorced and married tended to spend fewer days. The signifigance of this is unknown. The `Gender` categories show slightly higher median Initial_days for males and non binary patients and a notably lower median for females compared to males. The `Readmission` plot is very interesting. It shows a significantly higher median and and grouping of `Initial_days` for readmitted patients compared to non-readmitted patients, with several outliers showing long stays among readmitted patients. Curiously, this appears to also show two distinct groups. This is noted. Lastly, the Area plot demonstrates a slightly lower median for urban areas, suggesting hospital time is less for urban patients compared to rural and suburban patients. Interstingly, `Initial_admin` shows a higher median for elective admissions compared to emergency admissions.\n",
    "\n",
    "- *The red lines in the boxplots show the mean values for each group. This is more about practice with visualizations than anything and to quickly compare the mean to the median. If the mean is far away from the median, could suggests that the distribution of `Initial_days` within the category is skewed, possibly indicating the presence of outliers or a non-normal distribution. However, the skewness alone does not directly tell one that it is a poor candidate for the model. Additionally, the order of the arrangement of categories on the x-axis of can influence the interpretation and direction of the mean line trend. If the categories are arranged in a certain order, the mean line might appear to trend up, down, or remain flat. So it's important to not draw those kinds of conclusions from the mean line alone.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 by 2 subplot grid\n",
    "fig, axes = plt.subplots(4, 2, figsize=(8, 16))\n",
    "\n",
    "# S_T_Admission\n",
    "sns.boxplot(data=df, x='S_T_Admission', y='Initial_days', ax=axes[0, 0], color='lightblue')\n",
    "sns.pointplot(data=df, x='S_T_Admission', y='Initial_days', ax=axes[0, 0], color='brown', estimator=np.mean, errorbar=None)\n",
    "axes[0, 0].set_title('S_T_Admission vs. Initial_days')\n",
    "\n",
    "# S_T_Treatment\n",
    "sns.boxplot(data=df, x='S_T_Treatment', y='Initial_days', ax=axes[0, 1], color='lightblue')\n",
    "sns.pointplot(data=df, x='S_T_Treatment', y='Initial_days', ax=axes[0, 1], color='brown', estimator=np.mean, errorbar=None)\n",
    "axes[0, 1].set_title('S_T_Treatment vs. Initial_days')\n",
    "\n",
    "# S_T_Visits\n",
    "sns.boxplot(data=df, x='S_T_Visits', y='Initial_days', ax=axes[1, 0], color='lightblue')\n",
    "sns.pointplot(data=df, x='S_T_Visits', y='Initial_days', ax=axes[1, 0], color='brown', estimator=np.mean, errorbar=None)\n",
    "axes[1, 0].set_title('S_T_Visits vs. Initial_days')\n",
    "\n",
    "# S_Reliability\n",
    "sns.boxplot(data=df, x='S_Reliability', y='Initial_days', ax=axes[1, 1], color='lightblue')\n",
    "sns.pointplot(data=df, x='S_Reliability', y='Initial_days', ax=axes[1, 1], color='brown', estimator=np.mean, errorbar=None)\n",
    "axes[1, 1].set_title('S_Reliability vs. Initial_days')\n",
    "\n",
    "# S_Options\n",
    "sns.boxplot(data=df, x='S_Options', y='Initial_days', ax=axes[2, 0], color='lightblue')\n",
    "sns.pointplot(data=df, x='S_Options', y='Initial_days', ax=axes[2, 0], color='brown', estimator=np.mean, errorbar=None)\n",
    "axes[2, 0].set_title('S_Options vs. Initial_days')\n",
    "\n",
    "# S_Hours_Treatment\n",
    "sns.boxplot(data=df, x='S_Hours_Treatment', y='Initial_days', ax=axes[2, 1], color='lightblue')\n",
    "sns.pointplot(data=df, x='S_Hours_Treatment', y='Initial_days', ax=axes[2, 1], color='brown', estimator=np.mean, errorbar=None)\n",
    "axes[2, 1].set_title('S_Hours_Treatment vs. Initial_days')\n",
    "\n",
    "# S_Staff\n",
    "sns.boxplot(data=df, x='S_Staff', y='Initial_days', ax=axes[3, 0], color='lightblue')\n",
    "sns.pointplot(data=df, x='S_Staff', y='Initial_days', ax=axes[3, 0], color='brown', estimator=np.mean, errorbar=None)\n",
    "axes[3, 0].set_title('S_Staff vs. Initial_days')\n",
    "\n",
    "# S_Active_Listening\n",
    "sns.boxplot(data=df, x='S_Active_Listening', y='Initial_days', ax=axes[3, 1], color='lightblue')\n",
    "sns.pointplot(data=df, x='S_Active_Listening', y='Initial_days', ax=axes[3, 1], color='brown', estimator=np.mean, errorbar=None)\n",
    "axes[3, 1].set_title('S_Active_Listening vs. Initial_days')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Seeing these survey results in a bivariate plot with Initial_days is interesting in the variation in the median Initial_days across the different survey responses. Admittedly, I am a little unsure about how to interpret this. The initial thinking is that there is some interesting insights to gleam from this. Perhaps this suggests that the survey responses may have some predictive power in determining the length of a patient's hospital stay.\n",
    "\n",
    "- It is interesting to see the relationship between the highest survey responses and the length of hospital stay though, where it the highest rating in most categorys is correlated with the highest and lowest hospital stay lengths. This may be insightful to hospitals in terms of patient care and satisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_reduced.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C4 Data Transformation\n",
    "## Reexpression of categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since the dataset contains several nominal categorical variables, it is essential to re-express these variables in a numerical format to include them in the regression model. This process is known as one-hot encoding, and it involves re-expressing categorical variables as binary variables, a format the regression model can use, by creating dummy variables for each category within a categorical variable. The Pandas library provides a method for performing this transformation using the `pd.get_dummies()` function. This function creates a new binary column for each category in a categorical variable,; 1 indicating the presence of that category and 0 indicating the absence. The original categorical variable is then dropped from the dataset to avoid multicollinearity issues in the regression model.\n",
    "\n",
    "- Ordinal and binary variables *(Yes/No->1/0)* will be re-expressed as well using pythons `replace` method. \n",
    "\n",
    "- (`replace` is apparently being depreciated: \"*FutureWarning: Downcasting behavior in replace is deprecated and will be removed in a future version. To retain the old behavior, explicitly call result.infer_objects(copy=False). To opt-in to the future behavior, set pd.set_option('future.no_silent_downcasting, True)`df[binary_vars] = df[binary_vars].replace({'Yes': 1, 'No': 0})*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select and show values counts for binary variables to compare berfore and after reexpression \n",
    "binary_vars = [col for col in df.columns if df[col].isin(['Yes', 'No']).all()]\n",
    "for col in binary_vars:\n",
    "    print(df[col].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-expression of binary variables\n",
    "df[binary_vars] = df[binary_vars].replace({'Yes': 1, 'No': 0})\n",
    "\n",
    "# check the unique values for the binary variables\n",
    "for col in binary_vars:\n",
    "    print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data dictionary states: \"The (Survey) variables represent responses to an eight question survey asking customers to rate the importance of various factors/surfaces on a scale of 1 to 8 (1 most important, 8 least important)\" Generally 1 is low and 8 is high in terms of importance and having 1 as most important and 8 as least important is not intuitive, and has caused this analyst confusion. Therefore, we will reverse the scale of survey variables so that 1 is the lowest and 8 is the highest in terms of importance. This will make the interpretation more intuitive in the regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect values and compare before and after\n",
    "survey_vars = ['S_T_Admission', 'S_T_Treatment', 'S_T_Visits', 'S_Reliability', 'S_Options', 'S_Hours_Treatment', 'S_Staff', 'S_Active_Listening']\n",
    "df[survey_vars].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse the scale of survey variables so that 1 is the lowest and 8 is the highest in terms of importance. and after the change.\n",
    "df[survey_vars] = df[survey_vars].replace({1: 8, 2: 7, 3: 6, 4: 5, 5: 4, 6: 3, 7: 2, 8: 1})\n",
    "\n",
    "df[survey_vars].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reexpress `complication_risk` from categorical variable to numerical variable where 1 is the lowest and 3 is the highest in terms of risk.\n",
    "# Check values before and after\n",
    "print(df['Complication_risk'].value_counts())\n",
    "df['Complication_risk'] = df['Complication_risk'].replace({'Low': 1, 'Medium': 2, 'High': 3}).astype(int)\n",
    "df['Complication_risk'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to csv to save progress so far.\n",
    "df.to_csv('df_for_one_hot.csv', index='CaseOrder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the csv\n",
    "df = pd.read_csv('df_for_one_hot.csv', index_col=0)\n",
    "df.head().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   To handle nominal variables (categorical variables with no inherent order) in a regression model, one-hot encoding is often used. This transforms each unique category of a variable into a separate binary variable. Each new binary variable represents the presence (1) or absence (0) of the category for a data point. (Middleton, 2022)\n",
    "\n",
    "-   To avoid introducing multicollinearity, it's common practice to drop one of the binary variables from each encoded category. Which will be done with the optional argument `drop_first=True` in the `pd.get_dummies` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using get_dummies to convert nominal variables to 1 and 0 for one-hot encoding and drop the first column to avoid multicollinearity.\n",
    "nominal_vars = ['Area', 'Marital', 'Gender', 'Initial_admin']\n",
    "df_encoded = pd.get_dummies(df, columns=nominal_vars, dtype=int, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the head of the encoded DataFrame\n",
    "pd.set_option('display.max_columns', None)\n",
    "df_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Noting the shape to be sure extra columns drop: 38 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINAL CLEAN TRANSFORMED CSV\n",
    "df_encoded.to_csv('medical_transformed.csv', index='CaseOrder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import Transformed Data for Initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('medical_transformed.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a histogram of the initial_days column\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.hist(df['Initial_days'], bins=50)\n",
    "plt.xlabel('Initial_days')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV: Model Comparison and Analysis\n",
    "\n",
    "## D. Compare an initial and a reduced linear regression model by doing the following:\n",
    "\n",
    "### **D1. Construct an initial multiple linear regression model from all independent variables that were identified in part C2:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The processes and code below were informed by several source mentioned in the refrence section. (Sewell, 2024), (UnfoldDataScience YouTube, 2023), (Stack Overflow, 2020), (GeeksforGeeks, 2023), (Indhumathy Chelliah, 2021)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# muultiple regression model using df and ols and 'Initial_days' as the dependent variable and all other variables in the dataset as independent variables\n",
    "\n",
    "X = df.drop('Initial_days', axis=1)\n",
    "Y = df['Initial_days']\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(Y, X).fit()\n",
    "predictions = model.predict(X)\n",
    "model_summary = model.summary()\n",
    "\n",
    "model_summary\n",
    "# create a csv file with the model summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Regression Model based on all predictors\n",
    "\n",
    "-   **(Å·) = 19.4602 + 0.0401(Children) + 0.0035(Age) - 3.442e-06(Income) + 46.4505(ReAdmis) - 0.0775(VitD_levels) - 0.1714(Doc_visits) + 0.2924(vitD_supp) - 0.4475(HighBlood) - 0.2008(Stroke) - 0.3944(Complication_risk) - 0.2090(Overweight) + 0.6649(Arthritis) + 0.0132(Diabetes) - 0.3959(Hyperlipidemia) + 0.3505(BackPain) + 0.5303(Anxiety) + 0.4092(Allergic_rhinitis) + 0.4223(Reflux_esophagitis) + 0.0406(Asthma) + 0.4003(S_T_Admission) + 0.1342(S_T_Treatment) - 0.1296(S_T_Visits) + 0.3911(S_Reliability) + 0.0093(S_Options) - 0.2056(S_Hours_Treatment) - 0.2466(S_Staff) - 0.1981(S_Active_Listening) + 0.1602(Area_Suburban) + 0.3731(Area_Urban) - 0.0263(Marital_Married) + 0.4302(Marital_Never Married) + 0.7953(Marital_Separated) + 0.2762(Marital_Widowed) - 0.0963(Gender_Male) - 0.2836(Gender_Nonbinary) - 1.6011(Initial_admin_Emergency Admission) - 0.2463(Initial_admin_Observation Admission)**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **D2. Justify a statistically based feature selection procedure or a model evaluation metric to reduce the initial model in a way that aligns with the research question:**\n",
    "\n",
    "-  A `backwards selection` method will be used to reduce the initial model. To justify the feature selection, model summary statistics will be analyzed, certain assumptions will be checked, and visualizations will be created and analyzed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Note:** The following requirements from `Part E` of the performance assessment will be demonstrated in the multiple cells below, but not necessarly in the exact order of the PA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### **E.  Analyze the data set using your reduced linear regression model by doing the following:**\n",
    "\n",
    "- **E1.  Explain your data analysis process by comparing the initial multiple linear regression model and reduced linear regression model, including the following element:**\n",
    "\n",
    "     â€¢  a model evaluation metric\n",
    "\n",
    "- **E2.  Provide the output and all calculations of the analysis you performed, including the following elements for your reduced linear regression model:**\n",
    "\n",
    "     â€¢   a residual plot\n",
    "     â€¢   the modelâ€™s residual standard error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot actual vs predicted initial days\n",
    "sns.regplot(x=predictions, y=Y, fit_reg=True, line_kws={'color':'orange'}, ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted Initial Days')\n",
    "axes[0].set_ylabel('Actual Initial Days')\n",
    "axes[0].set_title('Actual vs Predicted Initial Days')\n",
    "\n",
    "# Plot histogram of actual values\n",
    "axes[1].hist(Y, bins=100)\n",
    "axes[1].set_xlabel('Initial Days')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Histogram of Actual Initial Days')\n",
    "\n",
    "# Plot histogram of predicted values\n",
    "axes[2].hist(predictions, bins=100)\n",
    "axes[2].set_xlabel('Predicted Initial Days')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].set_title('Histogram of Predicted Initial Days')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The plot of `Actual vs Predicted days` seems to miss much of the data in the middle, predicting at lower and higher values. The `actual Initial Days` histogram again shows it's bimodal distribution, suggesting two distinct groups or patterns within the data. The `predicted Initial Days` histogram clearly shows a large range of missing values in the middle, and is heavily concentrated at the low and high ends of the range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate RSE\n",
    "mse = model.scale\n",
    "# Calculate RSE\n",
    "rse = np.sqrt(mse)\n",
    "print(\"Residual Standard Error (RSE):\", rse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rse calculation: (Stack Overflow 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Model Fit:\n",
    "\n",
    "- The R-squared is 0.726, suggesting that approximately 72.6% of the variability in `Initial_days` may be explained by the model, which in combination with the almost identical Adj. R-squared of 0.725 indicating a good fit with initial model.\n",
    "- The F-statistic is 714.14 with a Prob (F-statistic) of 0.00, *suggesting* that the model is statistically significant overall.\n",
    "- The AIC 8.090e+04 and BIC 8.117e+04 are very similar, suggesting suggests that both are close in their evaluation of model complexity. These will be re-examined in the reduced model to see if they are lowered. \n",
    "- Residual Standard Error calculation (RSE): 13.79 `Initial_days` ranges from 1 to 72 days. An RSE of 13.79 days represents over 19% (13.792 / 72 * 100) of the total range. This suggests that, on average, the model's predictions for length of stay can deviate from the actual values by up to 13.792 days. This seems significant in this context and indicates room for improvement.\n",
    "\n",
    "Variables.\n",
    "Some predictor variables have high t-values and low p-values (P>|t|), sometimes indicating that they are statistically significant. Of note is the `ReAdmis` feature, it has a very low p-value and a highly significant coefficient (46.4505), suggesting a strong association with `Initial_days`\n",
    "However, variables such as Children, Age, Income, and others have high p-values, indicating that they might not be significant predictors of `Initial_days` in the presence of other variables.\n",
    "The const coefficient (y-intercept) is 19.5835, which represents the expected value of `Initial_days` when all other predictors are at zero.\n",
    "It is also important to note the values of the coefficients for the predictors. Larger values can suggest a more important role for the predictor in the model. For example, the coefficient for `ReAdmis` is 46.4505, which means that for every unit increase in `ReAdmis`, the expected value of `Initial_days` increases by 46.4505 units, holding all other predictors constant. This is a large coefficient compared to others, indicating a strong relationship between `ReAdmis` and `Initial_days`. \n",
    "\n",
    "- Overall, even though there are some summary statistics points to a reliable model, we should be skeptical with the warning about potential multicollinearity, and the plot of the actual vs predicted values not accounting for much of the data. There are several things we should check. \n",
    "\n",
    "Issues:\n",
    "The note on multicollinearity \"[2] The condition number is large, 8.89e+05. This might indicate that there are strong multicollinearity or other numerical problems.\" indicates that there might be high correlation between some predictors. This needs to be investigated. Checking the residuals and Variance inflation factor (VIF) analysis can help to identify and highly correlated predictors.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "residuals = model.resid\n",
    "# Plot the residuals\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
    "\n",
    "# reesiduals Dist\n",
    "sns.histplot(residuals, bins=30, ax=axes[0])\n",
    "axes[0].set_xlabel('Residuals')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Residuals Distribution')\n",
    "\n",
    "# mean and median lines to the histogram\n",
    "mean_residuals = residuals.mean()\n",
    "median_residuals = residuals.median()\n",
    "axes[0].axvline(x=mean_residuals, color='green', linestyle='--', label='Mean')\n",
    "axes[0].axvline(x=median_residuals, color='blue', linestyle='--', label='Median')\n",
    "axes[0].legend()\n",
    "\n",
    "# QQ plot of residuals\n",
    "sm.qqplot(residuals, line='s', ax=axes[1])\n",
    "axes[1].set_xlabel(\"Theoretical Quantiles\")\n",
    "axes[1].set_ylabel(\"Sample Quantiles\")\n",
    "axes[1].set_title(\"QQ plot of residuals\")\n",
    "\n",
    "plt.axhline(y=0, color='red', linestyle='--') \n",
    "sns.scatterplot(x=predictions, y=residuals, ax=axes[2])\n",
    "axes[2].set_xlabel(\"Predicted values\")\n",
    "axes[2].set_ylabel(\"Residuals\")\n",
    "axes[2].set_title(\"Residuals vs. predicted values\")\n",
    "\n",
    "# mean and median lines \n",
    "mean_residuals = residuals.mean()\n",
    "median_residuals = residuals.median()\n",
    "axes[2].axhline(y=mean_residuals, color='green', linestyle='--', label='Mean')\n",
    "axes[2].axhline(y=median_residuals, color='blue', linestyle='--', label='Median')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ideally, one would like to see a normal distribution centered around zero for the residual distribution, but this **histogram** indicates a slightly bimodal distribution that is skewed to the right, with a tail that grows slightly positive values. Hoever, the mean is in fact around 0, which presents and interesting challenge.\n",
    "\n",
    "- The **Q-Q plot** shows that the residuals are not normally distributed, as the points do not fall along the straight line. This plot required research to interpret as I was not familiar with it. From S. Kross as seankross.com: \"*The points in Q-Q plot then cross below the blue line indicating that the actual quantiles that are close to zero are farther from zero than they should be theoretically. At the center of the theoretical distribution there are no data in the actual dataset, and therefore there is no point in the Q-Q plot at (0, 0). The upper half of the Q-Q plot is a reflection across X and Y of the bottom half.*\" (Kross, 2016) The author also suggest this is the results of a residual distribution that is similar to the one I have. Additionally, the center of the theoretical distribution indeed did not have any data points as mentioned above.\n",
    "\n",
    "- According to The residuals (errors) should be scattered randomly above and below the zero line across the entire range of predicted value and There should be no discernible pattern in the scatter plot of residuals. This is not the case in my scatter plot of residuals. Indicating that the model is not a good fit for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VIF analysis will be used to identify highly correlated predictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform VIF analysis to check for multicollinearity\n",
    "X = add_constant(X)\n",
    "vif = pd.DataFrame()\n",
    "vif[\"variables\"] = X.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n",
    "vif.sort_values(by='VIF', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the VIF analysis, most variables have VIF values well below 5, indicating no significant multicollinearity among them, which is surprising given the message from the model summary: *\"[2] The condition number is large, 8.89e+05. This might indicate that there are strong multicollinearity or other numerical problems.\"* The highest VIF values observed for `S_T_Admission`, `S_T_Treatment`, and `marital` status categories, but even these do not exceed the threshold of 5, suggesting moderate correlation at most. (Stack Exchange, 2012)\n",
    "\n",
    "Given the generally low VIF values, the summary statistics mentioned above the suggest a good model fit, the residuals and Q-Q plots, and the actual and predicted values, something less obvoius must be wrong with the initial model. It is also important to keep in mind the bimodal distribution of the actual `Initial_days` values, which may be contributing to the model's poor performance due to the violation of the assumption of normality.\n",
    "\n",
    "One option is to try to transform the `Initial_days` variable to make it more normally distributed. This could involve taking the log of the variable, or splitting the data into two groups based on the bimodal distribution and creating separate models for each group.(Bradley, 2023) However, this would likely be complex at this point in the process and may not be necessary if a reduced model can be created that performs better than the initial model. Therefore, before that is attempted, I will employ what *domain knowledge* I have regarding the variable and use a *correlation matrix* to help identify pairwise relationships between the independent variable. In this context, perhaps a better model can be created by removing some of the predictors that are not significantly associated with `Initial_days` and may be contributing to the model's poor performance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign independent variables\n",
    "corr_matrix = df.drop('Initial_days', axis=1).corr()\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# correlation matrix with  values and adjusted font size\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, annot_kws={\"size\": 8}, vmin=-1, vmax=1)\n",
    "\n",
    "plt.title('Correlation Matrix of Independent Variables')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The correlation matrix shows that most of the variables that contain any correlation wit each other are the survey items, marital status, area, and initial admission. These would be interesting to explore further but in the context of this analyst, are not as useful as the other predictors which are largely health and biological factors and importantly, readmission status. The survey items are likely to be highly correlated with each other, and subjective feedback worthy of their own separate analyses, but here may be getting in the way of the model.Marital status, in my personal experience, has never been a factor health care providers consider except when contacting family members. I feel that these are good candidates for removal from the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['S_T_Admission', 'S_T_Treatment', 'Marital_Widowed', 'Marital_Married', 'Marital_Never Married', 'Marital_Separated', 'S_T_Visits', 'S_Hours_Treatment', 'S_Reliability', 'S_Staff','S_Options', 'Area_Urban','Initial_admin_Observation Admission', 'S_Active_Listening', 'Area_Suburban','Initial_admin_Emergency Admission'], axis=1)\n",
    "df.head().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('medical_transformed_drop1.csv', index='CaseOrder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the new csv file\n",
    "df = pd.read_csv('medical_transformed_drop1.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Initial_days'], axis=1)\n",
    "Y = df['Initial_days']\n",
    "X = sm.add_constant(X)\n",
    "model_2 = sm.OLS(Y, X).fit()\n",
    "predictions = model_2.predict(X)\n",
    "residuals_2 = model_2.resid\n",
    "model_summary_2 = model_2.summary()\n",
    "model_summary_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate RSE\n",
    "mse = model_2.scale\n",
    "# Calculate RSE\n",
    "rse = np.sqrt(mse)\n",
    "print(\"Residual Standard Error (RSE):\", rse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The R-squared went from 0.726 to 0.725 \n",
    "- Adj. R-squared went from 0.725 to 0.724\n",
    "- The F-statistic went from 714.14 with a Prob (F-statistic) of 0.0 to 1314 with a Prob (F-statistic) of 0.0. The f-statistic increase suggests that the model is a better fit than the previous model.\n",
    "- The AIC and BIC went from 8.090e+04 and 8.117e+04 to AIC 8.092e+04 and BIC 8.107e+04 are very similar to the previous model.\n",
    "- Residual Standard Error calculation (RSE): 13.79 to 13.8, almost unchanged\n",
    "- The condition number went from 8.88e+05 to 5.64e+05 which is a significant improvement, suggesting that the multicollinearity is less of an issue in this model. But still the condition number is high, so multicollinearity may still be a problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the residuals_2\n",
    "residuals_2_2 = model_2.resid\n",
    "\n",
    "# Plot the residuals_2\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
    "\n",
    "# iduals_2 Distribution\n",
    "sns.histplot(residuals_2, bins=30, ax=axes[0])\n",
    "axes[0].set_xlabel('Residuals_2residuals_2')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Residuals_2residuals_2 Distribution')\n",
    "\n",
    "#mean and median lines \n",
    "mean_residuals_2 = residuals_2.mean()\n",
    "median_residuals_2 = residuals_2.median()\n",
    "axes[0].axvline(x=mean_residuals_2, color='green', linestyle='--', label='Mean')\n",
    "axes[0].axvline(x=median_residuals_2, color='blue', linestyle='--', label='Median')\n",
    "axes[0].legend()\n",
    "\n",
    "# QQ plot\n",
    "sm.qqplot(residuals_2, line='s', ax=axes[1])\n",
    "axes[1].set_xlabel(\"Theoretical Quantiles\")\n",
    "axes[1].set_ylabel(\"Sample Quantiles\")\n",
    "axes[1].set_title(\"QQ plot of residuals_2\")\n",
    "\n",
    "plt.axhline(y=0, color='red', linestyle='--') \n",
    "sns.scatterplot(x=predictions, y=residuals_2, ax=axes[2])\n",
    "axes[2].set_xlabel(\"Predicted values\")\n",
    "axes[2].set_ylabel(\"Residuals_2\")\n",
    "axes[2].set_title(\"Residuals_2 vs. predicted values\")\n",
    "\n",
    "# Add mean and median lines \n",
    "mean_residuals_2 = residuals_2.mean()\n",
    "median_residuals_2 = residuals_2.median()\n",
    "axes[2].axhline(y=mean_residuals_2, color='green', linestyle='--', label='Mean')\n",
    "axes[2].axhline(y=median_residuals_2, color='blue', linestyle='--', label='Median')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Not much has changed in our residual plots. Although our model is slightly improved according to the F-statistic, the R-squared and the condition number, the residual plots are still showing some patterns that suggest that the model is not capturing all the information in the data.\n",
    "\n",
    "- Let's check VIF values for each of the predictors again to see if we gain ant new insights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform VIF analysis to check for multicollinearity\n",
    "X = add_constant(X)\n",
    "vif = pd.DataFrame()\n",
    "vif[\"variables\"] = X.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n",
    "vif.sort_values(by='VIF', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The VIF values for the predictors are all essentially 1. \n",
    "- Given this information, re-examining the p-values of the predictors and their coefficients, we can eliminate the predictors with the highest p-values and lower coefficients  to see if that improves the model. Here we have to be careful, because predictors with high p-values may still be important for the model. Examining the p-values and the coefficients together, \n",
    "\n",
    "- After reviewing p-values and coefficients, I am choosing to remove based on coefficient as it is associated with higher p-values. \n",
    "- Eliminate those variables with `coefficients less than an absolute value of 0.4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistically significant variables\n",
    "significant_vars = model_2.params[model_2.params.abs() > 0.4].index.tolist()\n",
    "\n",
    "# Remove 'const' from the list\n",
    "if 'const' in significant_vars:\n",
    "    significant_vars.remove('const')\n",
    "\n",
    "print('Significant variables:', significant_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reduced model with only significant variables above\n",
    "X_reduced = X[significant_vars]\n",
    "\n",
    "# Fit the OLS model with reduced variables\n",
    "model_reduced = sm.OLS(Y, sm.add_constant(X_reduced)).fit()\n",
    "\n",
    "# Print the summary of the reduced model\n",
    "model_reduced.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate RSE\n",
    "mse = model_reduced.scale\n",
    "# Calculate RSE\n",
    "rse = np.sqrt(mse)\n",
    "print(\"Residual Standard Error (RSE):\", rse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new residual check and assign\n",
    "residuals_reduced = model_reduced.resid\n",
    "residuals_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the residuals_reduced\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
    "\n",
    "# residuals_reduced Distribution\n",
    "sns.histplot(residuals_reduced, bins=30, ax=axes[0])\n",
    "axes[0].set_xlabel('residuals_reduced')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('residuals_reduced Distribution')\n",
    "\n",
    "#  mean and median lines  the histogram\n",
    "mean_residuals_reduced = residuals_reduced.mean()\n",
    "median_residuals_reduced = residuals_reduced.median()\n",
    "axes[0].axvline(x=mean_residuals_reduced, color='green', linestyle='--', label='Mean')\n",
    "axes[0].axvline(x=median_residuals_reduced, color='blue', linestyle='--', label='Median')\n",
    "axes[0].legend()\n",
    "\n",
    "# QQ plot of residuals_reduced\n",
    "sm.qqplot(residuals_reduced, line='s', ax=axes[1])\n",
    "axes[1].set_xlabel(\"Theoretical Quantiles\")\n",
    "axes[1].set_ylabel(\"Sample Quantiles\")\n",
    "axes[1].set_title(\"QQ plot of residuals_reduced\")\n",
    "\n",
    "plt.axhline(y=0, color='red', linestyle='--') \n",
    "sns.scatterplot(x=predictions, y=residuals_reduced, ax=axes[2])\n",
    "axes[2].set_xlabel(\"Predicted values\")\n",
    "axes[2].set_ylabel(\"residuals_reduced\")\n",
    "axes[2].set_title(\"residuals_reduced vs. predicted values\")\n",
    "\n",
    "# Add mean and median \n",
    "mean_residuals_reduced = residuals_reduced.mean()\n",
    "median_residuals_reduced = residuals_reduced.median()\n",
    "axes[2].axhline(y=mean_residuals_reduced, color='green', linestyle='--', label='Mean')\n",
    "axes[2].axhline(y=median_residuals_reduced, color='blue', linestyle='--', label='Median')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### D3.  Provide a reduced linear regression model that follows the feature selection or model evaluation process in part D2, including a screenshot of the output for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduced model\n",
    "- The R-squared went from 0.726 to 0.725 to `0.724`\n",
    "- Adj. R-squared went from 0.725 to 0.724 to `0.724` as well.\n",
    "- The F-statistic went from 714.14 to 1314 to `5252` all with a Prob (F-statistic) of 0.0. The f-statistic increase suggests that the model is a much better fit than the previous model.\n",
    "- The AIC and BIC went from 8.090e+04 and 8.117e+04 to AIC 8.092e+04 and BIC 8.107e+04 to `AIC 8.090e+04` and `BIC 8.095e+04`, very close to each other.\n",
    "- Residual Standard Error calculation (RSE): 13.79 to remain at `13.8` no matter the model change.\n",
    "- The condition number went from 8.88e+05 to 5.64e+05 to `3.63` suggesting that the multicollinearity is less of an issue in this model. Likely helped by the remove of features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### (Å·) = 17.3266 + 46.4413(ReAdmis) - 0.4551(HighBlood) + 0.6742(Arthritis) - 0.4176(Hyperlipidemia) + 0.5454(Anxiety)\n",
    "\n",
    "- **ReAdmis (46.4413)**: This coefficient is positive and the largest in magnitude. This suggests that a history of readmission (ReAdmis = 1) has the strongest positive association with the predicted outcome (Å·). In other words, having a history of readmission is predicted to increase the value of Å·.\n",
    "\n",
    "- **HighBlood (-0.4551)**: This coefficient is negative. A negative coefficient indicates an inverse relationship between HighBlood and the predicted outcome. So, having high blood pressure (HighBlood = 1) is predicted to decrease the value of Å·.\n",
    "\n",
    "- **Arthritis (0.6742)**: This coefficient is positive, indicating a positive association between arthritis and the predicted outcome. So, having arthritis (Arthritis = 1) is predicted to increase the value of Å·.\n",
    "\n",
    "- **Hyperlipidemia (-0.4176)**: Similar to HighBlood, this coefficient is negative. So, having hyperlipidemia (high cholesterol) is predicted to decrease the value of Å·.\n",
    "\n",
    "- **Anxiety (0.5454)**: This coefficient is positive, indicating a positive association between anxiety and the predicted outcome. So, having anxiety (Anxiety = 1) is predicted to increase the value of Å·.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions_reduced = model_reduced.predict(sm.add_constant(X_reduced))\n",
    "actual_vs_predicted = pd.DataFrame({'Actual': Y, 'Predicted': predictions_reduced})\n",
    "# Plot the actual vs predicted values\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.regplot(x='Predicted', y='Actual', data=actual_vs_predicted, fit_reg=True, line_kws={'color':'orange'})\n",
    "plt.show()\n",
    "\n",
    "#boxplot of the of initial_days and readmis\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.boxplot(data=df, x='ReAdmis', y='Initial_days', color='lightgreen')\n",
    "sns.pointplot(data=df, x='ReAdmis', y='Initial_days', color='red', estimator=np.mean, errorbar=None)\n",
    "plt.title('Boxplot for ReAdmis and Initial_days')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-   This project was a remarkable learning experience, shaped significantly by the guidance of Dr. Straw and Dr. Middleton's advice. Dr. Straw indicated that the project would force tough decisions, highlighting the absence of standout models and the essential nature of hard choices. Dr. Middleton emphasized the importance of casting a wide net in selecting variables for the initial model. Following their advice, I embraced the challenge of making difficult decisions and applied a broad approach in my variable selection.\n",
    "\n",
    "-   As I reached the project's conclusion, I recognized possible mistakes and oversights along the way. Despite the urge to correct these errors, I opted to keep them, valuing the learning process over the creation of a flawless model as well as the reality of deadlines. This provides a blueprint of my thinking, offering a reminder of overlooked aspects crucial for future projects, particularly in the Multiple Linear Irrigation case study.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "\n",
    "- Datacamp. (2023, December 12). D207 - Exploratory Data Analysis. Retrieved from https://app.datacamp.com/learn/custom-tracks/custom-d207-exploratory-data-analysis \n",
    "\n",
    "- Middleton, K. (2022, November). Getting started with D208 Part I [Webinar]. Western Governors University. Retrieved March 2024, from https://wgu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=15e09c73-c5aa-439d-852f-af47001b8970\n",
    "\n",
    "- Eyre, I. (2024, March). Python Seaborn: A Guide to Using Seaborn for Data Visualization. Real Python. Retrieved March 2024, from https://realpython.com/python-seaborn/\n",
    "\n",
    "- Statology. (n.d.). *The Five Assumptions of Multiple Linear Regression*. Statology. Retrieved March 10, 2024, from www.statology.org/multiple-linear-regression-assumptions/\n",
    "\n",
    "- GeeksforGeeks. (n.d.). Interpreting the results of linear regression using OLS summary. Retrieved March 24, 2024, from https://www.geeksforgeeks.org/interpreting-the-results-of-linear-regression-using-ols-summary/\n",
    "\n",
    "- Indhumathy Chelliah. (2021, August 3). Everything to know about residuals in linear regression. Retrieved March 24, 2024, from https://indhumathychelliah.com/2021/08/03/everything-to-know-about-residuals-in-linear-regression/\n",
    "\n",
    "- Kross, S. (2016, February 29). A Q-Q Plot Dissection Kit. Retrieved March 24, 2024, from https://seankross.com/2016/02/29/A-Q-Q-Plot-Dissection-Kit.html\n",
    "\n",
    "- Bradley, C. (2023). How do you transform a skewed bimodal data set into a normal distribution? Retrieved March 24, 2024, from https://www.kaggle.com/code/chrisbradley/tab-playground-predicting-bimodal-distribution\n",
    "\n",
    "- Manl, F. (n.d.). Intermediate Regression with statsmodels in Python. Retrieved March 24, 2024, from https://github.com/FraManl/DataCamp/blob/main/Intermediate%20Regression%20with%20statsmodels%20in%20Python.ipynb\n",
    "\n",
    "- Python Graph Gallery. (n.d.). Retrieved March 24, 2024, from https://python-graph-gallery.com/\n",
    "\n",
    "- ResearchGate. (n.d.). Normal probability plots. Retrieved March 24, 2024, from https://www.researchgate.net/figure/Normal-pro-bability-plots-a-ideal-b-heavy-tailed-distri-bution-c-light-tailed_fig1_262663278\n",
    "\n",
    "- Statology. (n.d.). Multiple Linear Regression Assumptions. Retrieved March 24, 2024, from https://www.statology.org/multiple-linear-regression-assumptions/\n",
    "\n",
    "- Stack Exchange. (n.d.). Multicollinearity: When individual regressions are significant but VIFs are low. Retrieved March 24, 2024, from https://stats.stackexchange.com/questions/24464/multicollinearity-when-individual-regressions-are-significant-but-vifs-are-low\n",
    "\n",
    "- Stack Overflow. (2020, August 10). Residual standard error of a regression in Python. Retrieved March 24, 2024, from https://stackoverflow.com/questions/63333999/residual-standard-error-of-a-regression-in-python\n",
    "\n",
    "- Western Governors University. (n.d.). R or Python? Retrieved March 24, 2024, from https://www.wgu.edu/online-it-degrees/programming-languages/r-or-python.html\n",
    "\n",
    "- @UnfoldDataScience YouTube. (n.d.). Checking MLR Assumptions. Retrieved March 24, 2024, from https://www.youtube.com/watch?v=_XAurJJQ7jw\n",
    "\n",
    "- Sewell, W. (2024). [PowerPoint slides on model tuning]. Western Governors University. Retrieved March 24, 2024, from https://westerngovernorsuniversity-my.sharepoint.com/:p:/g/personal/william_sewell_wgu_edu/ERPQ0YpiQktOl-7YyAVnfLMBR5qeBh2cSv61VaJqe_aHKg?e=FjPhPz\n",
    "\n",
    "\n",
    "(GeeksforGeeks, 2023)\n",
    "(Indhumathy Chelliah, 2021)\n",
    "(Kross, 2016)\n",
    "(LinkedIn Learning, n.d.)\n",
    "(Manl, n.d.)\n",
    "(Python Graph Gallery, n.d.)\n",
    "(ResearchGate, n.d.)\n",
    "(Statology, n.d.)\n",
    "(Stack Exchange, n.d.)\n",
    "(Stack Overflow, 2020)\n",
    "(Western Governors University, n.d.)\n",
    "(UnfoldDataScience YouTube, 2023)\n",
    "(Sewell, 2024, slide 5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Software\n",
    "\n",
    "The following software packages were used in this project:\n",
    "\n",
    "* **pandas** pandas is a Python library providing data structures and data analysis tools.\n",
    "* **numpy** NumPy is a Python library for scientific computing.\n",
    "* **matplotlib** Matplotlib is a Python library for creating static, animated, and interactive visualizations.\n",
    "* **seaborn** Seaborn is a Python library for statistical data visualization built on top of matplotlib.\n",
    "* **statsmodels** Statsmodels is a Python library for statistical modeling and econometrics.\n",
    "* **Scikit-learn**  Scikit-learn is a free machine learning library for Python.\n",
    "\n",
    "**References**\n",
    "\n",
    "- Python Software Foundation. (2023). Python Language Reference, version 3.9.6. Retrieved from https://www.python.org\n",
    "\n",
    "- Scikit-Learn:\n",
    "  Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., ... & Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. *Journal of Machine Learning Research, 12*, 2825-2830. Retrieved from https://scikit-learn.org\n",
    "\n",
    "- Jinja2:\n",
    "  Ronacher, A. (2023). Jinja2: The modern and designer-friendly templating engine for Python. Retrieved from https://jinja.palletsprojects.com/\n",
    "\n",
    "- Matplotlib:\n",
    "  Hunter, J. D. (2007). Matplotlib: A 2D graphics environment. *Computing in Science & Engineering, 9*(3), 90-95. Retrieved from https://matplotlib.org\n",
    "\n",
    "- Statsmodels:\n",
    "  Seabold, S., & Perktold, J. (2010). Statsmodels: Econometric and statistical modeling with python. In *Proceedings of the 9th Python in Science Conference* (Vol. 57, p. 61). Retrieved from https://www.statsmodels.org\n",
    "\n",
    "- Pandas:\n",
    "  McKinney, W. (2010). Data Structures for Statistical Computing in Python. In *Proceedings of the 9th Python in Science Conference* (pp. 51-56). Retrieved from https://pandas.pydata.org\n",
    "\n",
    "- NumPy:\n",
    "  Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., ... & Oliphant, T. E. (2020). Array programming with NumPy. *Nature, 585*(7825), 357-362. Retrieved from https://numpy.org\n",
    "\n",
    "- Seaborn:\n",
    "  Waskom, M. (2021). Seaborn: statistical data visualization. *Journal of Open Source Software, 6*(60), 3021. Retrieved from https://seaborn.pydata.org\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manage memory by using gc.collect() to clear memory\n",
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations\n",
    "\n",
    "Beware of the following with your regression analysis:\n",
    "\n",
    "Overfitting can occur due to limited data points.\n",
    "\n",
    "Multicollinearity occurs when high association (correlation) with other IVs.\n",
    "\n",
    "P-values can be unreliable and coefficients swing wildly\n",
    "\n",
    "Check for pairwise correlations and high VIF (> 10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
