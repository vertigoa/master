{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Performance Assessment: D208 Predictive Modeling Task 1 - Multiple Linear Regression.\n",
    "\n",
    "## Michael Hindes\n",
    "Department of Information Technology, Western Governors University\n",
    "<br>D208: Predictive Modeling\n",
    "<br>Professor David Gagner\n",
    "<br>February 11, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to understanding the exact relationship between a response and predictor variables and to create a multiple regression model derived from medical raw data, targeting a business question reflective of a real-world organizational challenge. Python is employed to conduct a multiple regression analysis to explore the research question thoroughly. The analysis is supported by visual aids. The process also involves meticulous data cleaning to ensure accuracy and reliability. Additionally, the project shares the code used for the regression analysis, predictions and cleaning and wrangling. It concludes by detailing the regression equation, evaluating the statistical and practical significances, discussing limitations, and suggesting possible actions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Research Question\n",
    "## Describe the purpose of this data analysis by doing the following::\n",
    "\n",
    "### **A1. Research Question:**\n",
    "**\"A1. Research Question:\n",
    "\"What factors contribute to the length of a patient's hospital stay?\"**\n",
    "\n",
    "This question aims to identify key variables within the dataset that influence `Initial_days`, including possible financial aspects, services rendered, and patient risk factors. Understanding the primary drivers behind the length of a patient's hospital stay is crucial for multiple reasons. First, it allows healthcare providers to identify potential areas for improving operational efficiency, such as optimizing bed utilization and reducing wait times for incoming patients. Second, by recognizing which factors significantly impact hospital stay durations, healthcare professionals can tailor patient care plans more effectively, potentially leading to improved patient outcomes and satisfaction. Third, insights from this analysis can inform hospital administration decisions regarding resource allocation, staffing, and policy development, ensuring that the healthcare facility can better manage its resources while maintaining or enhancing the quality of care provided to patients. Lastly, given previous analyses on the dataset, there may be certain correlation between the number of days patient was hospitalized and his higher risk for readmission, which is costly to hospitals. Thus, the goal of this research is not only to uncover the underlying factors affecting hospital stays but also to leverage this understanding to facilitate better hospital management practices and improve overall patient care.\n",
    "\n",
    "\n",
    "\n",
    "### **A2. Define the goals of the data analysis.**\n",
    "\n",
    "This data analysis project is focused on developing a predictive model as a practical tool to help healthcare organizations in planning, patient care, and operational improvements. By examining a wide range of factors that potentially affect Initial_days, the project aims to understand any relationships initial_days may have with other variables. With that understanding, it seeks to build a model that supports data-driven decision-making in healthcare. The goals of the data analysis are as follows:\n",
    "\n",
    "- Ensure Data Quality and Integrity: Prioritize data cleaning and preprocessing to ensure the dataset's accuracy, completeness, and consistency. This involves identifying and addressing missing values, outliers, and errors in the data. A clean dataset is fundamental for reliable analysis and modeling, enabling more accurate predictions and insights.\n",
    "\n",
    "- Identify Key Predictors: Determine which variables significantly influence the length of hospital stays, considering a broad spectrum of factors, including but not limited to demographic information, medical history, financial aspects, and services received during the stay.\n",
    "\n",
    "- Quantify Relationships: Establish how selected variables are related to Initial_days, quantifying the strength and nature of these relationships to provide actionable insights.\n",
    "\n",
    "- Develop a Predictive Model: Create a multiple linear regression model that can accurately predict the length of a patient's hospital stay based on the identified variables, aiding in the anticipation of hospital capacity needs.\n",
    "\n",
    "- Inform Policy Making: Provide evidence-based recommendations to healthcare administrators and policymakers for developing policies that address the key factors contributing to the length of hospital stays, with the goal of improving overall healthcare efficiency and patient satisfaction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "project",
     "sectionA"
    ]
   },
   "source": [
    "# Part II: Method Justification\n",
    "\n",
    "## B. Describe multiple linear regression methods by doing the following:\n",
    "\n",
    "### **B1. Summarize four assumptions of a multiple linear regression model:**\n",
    "\n",
    "In multiple linear regression analysis, four key assumptions are critical: linearity between variables, independence of observations, constant error variance (homoscedasticity), and normal distribution of error terms. Understanding and checking these assumptions is essential for the model's reliability and accuracy, providing a solid basis for predictive analytics.\n",
    "\n",
    "-   **Linearity** asserts that there is a straight-line relationship between each predictor (independent variable) and the response (dependent variable). This means that changes in a predictor variable are associated with proportional changes in the response variable.\n",
    "\n",
    "-   **Independence of Observations** indicates that the data points in the dataset do not influence each other. Each observation's response is determined by its predictor values, free from the effects of other observations in the dataset.\n",
    "\n",
    "-   **Homoscedasticity** refers to the requirement that the error terms (differences between observed and predicted values) maintain a consistent variance across all levels of the independent variables. This constant variance ensures that the model's accuracy does not depend on the value of the predictors.\n",
    "\n",
    "-    **Normality of Errors** involves the assumption that for any fixed value of an independent variable, the error terms are normally distributed. This normal distribution is central to conducting various statistical tests on the model's coefficients to determine their significance.\n",
    "\n",
    "(Statology, n.d.)\n",
    "(Pennsylvania State University, n.d.)\n",
    "\n",
    "### **B2. Describe two benefits of using Python for data analysis:**\n",
    "\n",
    "- **Rich Libraries:** While R was specifically designed with statistics and data analysis in mind, Python distinguishes itself with its comprehensive suite of libraries that cater to virtually every phase of the data analysis process. Libraries such as Pandas for data manipulation allow for efficient handling and transformation of data, NumPy for numerical computations supports complex mathematical operations with ease, and Matplotlib along with Seaborn for visualization enable the creation of insightful and high-quality graphs and charts. Moreover, Scikit-learn offers a robust platform for applying machine learning algorithms, streamlining the development of predictive models. These libraries not only facilitate a wide range of data analysis tasks but also ensure that analysts have the tools needed to tackle complex data challenges effectively.\n",
    "\n",
    "- **Versatility** Python's syntax is known for its intuitive and readable nature, making it an accessible choice for professionals across various domains, from data science to web development. This versatility extends Python's utility beyond data analysis to other applications such as web development, automation, and deep learning, through frameworks and libraries like Flask, Selenium, and TensorFlow respectively. For instance, an analyst can easily switch from analyzing data to deploying a machine-learning model as a web application within the same programming environment. This seamless integration across different tasks enables a smooth workflow and promotes a holistic approach to problem-solving in today's interconnected digital landscape.\n",
    "\n",
    "### **B3. Explain why multiple linear regression is an appropriate technique for analyzing the research question summarized in part I:**\n",
    "\n",
    "Multiple linear regression is particularly suited for addressing the research question at hand, as it facilitates the exploration of how several independent variables collectively influence a single continuous dependent variable, in this case, `Initial_days`. This analytical technique is adept at not only identifying but also quantifying the strength and nature of the relationships between Initial_days and various predictors, such as financial aspects, services rendered, and patient risk factors. By accounting for multiple factors simultaneously, multiple linear regression can provide nuanced insights into their combined effects on the length of a hospital stay. This comprehensive understanding is crucial for building robust predictive models that can inform decision-making processes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEEDS EDIT\n",
    " Part III: Data Preparation\n",
    "\n",
    "## C. Summarize the data preparation process for multiple linear regression analysis by doing the following:\n",
    "\n",
    "### *C1. Describe your data cleaning goals and the steps used to clean the data to achieve the goals that align with your research question including your annotated code.**\n",
    "\n",
    "*   **Importing the Data**: Use`pd.read_csv()` to import data into a Pandas DataFrame.\n",
    "    \n",
    "*   **Initial Data Examination**: Using `df.head()` provides a quick snapshot of the dataset, including a view of the first few rows. This helps in getting a preliminary understanding of the data's structure and content.\n",
    "    \n",
    "*   **Checking Data Types**: The `df.info()` method is used for assessing the dataset's overall structure, including the data types of each column and the presence of non-null values. \n",
    "    \n",
    "*   **Identifying Duplicate Rows**: Utilizing `df.duplicated()` to find duplicate rows is an essential cleaning step. Duplicates can skew your analysis and lead to inaccurate models. Once identified, you can decide whether to remove these rows with `df.drop_duplicates()` depending on their relevance to your research question.\n",
    "    \n",
    "*   **Detecting Missing Values**: The `df.isnull().sum()` command is instrumental in identifying missing values across the dataset. Understanding where and how much data is missing is critical for deciding on imputation methods or if certain rows/columns should be excluded from the analysis.\n",
    "\n",
    "*   **Outlier Management Strategy:**: In this dataset, outliers are important to detect and be aware of. Particularly when creating predictive regression models. For example outliers can have a detrimental effect on our regression assumption and the model itself(MIDDLETON VIDEO) If there are outliers present, make sure that they are real is important. However, in the context of medical data, outliers can have an extra level of nuance. This is because in medical data, outliers are often the very things that we are interested in. For example, a patient with a very high cholesterol level or a very low blood pressure. These values are not errors, but rather important indicators of health conditions.\n",
    "\n",
    "- Therefore, in this analysis, we will look for outliers and pay close attention to the details of the variables themselves. Outliers will be noted, but not necessarily treated unless they are obvious data entry errors or if they hinder our model.\n",
    "\n",
    "*    **Reviewing Unique Values**: Although `df.unique()` is used to explore unique values in a Series, for dataframes, you might consider `df.nunique()` to see the number of unique values in each column or use `df['column_name'].unique()` to check unique values in specific columns. This step is valuable for understanding the diversity of information within your dataset, particularly for categorical data.\n",
    "\n",
    "-Wrangling\n",
    "\n",
    "*   **Drop Unnecessary Columns**: Any columns that are not relevant to the research question or the predictive model will be dropped from the dataset. \n",
    "\n",
    "*   **Categorical variable conversion**: Categorical variables will be transformed into numerical formats. Demographic data, which represents static information about patients and cannot be altered by the hospital, will be excluded from the analysis. We will identify and address any missing data, ensuring its proper mitigation. Additionally, any duplicate records identified in the dataset will be eliminated.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "follow the slides here: https://westerngovernorsuniversity.sharepoint.com/:p:/r/sites/DataScienceTeam/_layouts/15/Doc.aspx?sourcedoc=%7B285C378F-8089-4758-9ABE-29976D079B56%7D&file=Dr.%20Sewell%20D208_Predictive_Modeling_Webinar_Episode%201t.pptx&action=edit&mobileredirect=true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and libraries\n",
    "%pip install scikit-learn\n",
    "%pip install Jinja2\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Display variable description and data types with examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data variable description and data types with examples.\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(filename='variable_description.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data and read it into a dataframe, setting the first column CaseOrder as the index\n",
    "\n",
    "df_medical = pd.read_csv('D208_templates/medical_clean.csv', index_col=0)\n",
    "\n",
    "# Display the first five rows of the data\n",
    "df_medical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the last 5 rows of the dataframe\n",
    "df_medical.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the DataFrame information\n",
    "df_medical.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows. \n",
    "print(df_medical.duplicated().value_counts())\n",
    "\n",
    "# Display the count of duplicate rows\n",
    "print('Total Duplicated Rows: ', df_medical.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "df_medical.isnull().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C2.  EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns Item 1 to Item 8 to the appropriate column names. The 'S_' modifier is used to indicate the column is a survey item.\n",
    "new_col_names={\n",
    "    'Item1':'S_T_Admission',\n",
    "    'Item2':'S_T_Treatment', \n",
    "    'Item3':'S_T_Visits', \n",
    "    'Item4':'S_Reliability', 'Item5':'S_Options', \n",
    "    'Item6':'S_Hours_Treatment', \n",
    "    'Item7':'S_Staff', \n",
    "    'Item8':'S_Active_Listening'}\n",
    "df_medical.rename(columns=new_col_names, inplace=True)\n",
    "df_medical.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the data types and unique values count into a DataFrame easy reference and comparison\n",
    "data_types = df_medical.dtypes\n",
    "\n",
    "unique_values = df_medical.nunique()\n",
    "\n",
    "comparison_df = pd.DataFrame({'Data Type': data_types, 'Unique Values': unique_values})\n",
    "\n",
    "comparison_df.sort_values(by='Unique Values', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cardinality and Data Type Summary of Variables\n",
    "\n",
    "## Ratio Variables (Continuous with an absolute zero)\n",
    "- `Income`: 9993 unique values (float64)\n",
    "- `VitD_levels`: 9976 unique values (float64)\n",
    "- `Initial_days`: 9997 unique values (float64)\n",
    "- `TotalCharge`: 9997 unique values (float64)\n",
    "- `Additional_charges`: 9418 unique values (float64)\n",
    "- `Population`: 5951 unique values (int64)\n",
    "- `Children`: 11 unique values (int64)\n",
    "- `Age`: 72 unique values (int64)\n",
    "- `Doc_visits`: 9 unique values (int64)\n",
    "- `Full_meals_eaten`: 8 unique values (int64)\n",
    "- `vitD_supp`: 6 unique values (int64)\n",
    "\n",
    "## Interval Variables (Continuous without an absolute zero)\n",
    "- `Lat`: 8588 unique values (float64)\n",
    "- `Lng`: 8725 unique values (float64)\n",
    "\n",
    "## Ordinal Variables\n",
    "- `S_T_Admission`: 8 unique values (int64)\n",
    "- `S_T_Treatment`: 7 unique values (int64)\n",
    "- `S_T_Visits`: 8 unique values (int64)\n",
    "- `S_Reliability`: 7 unique values (int64)\n",
    "- `S_Options`: 7 unique values (int64)\n",
    "- `S_Hours_Treatment`: 7 unique values (int64)\n",
    "- `S_Staff`: 7 unique values (int64)\n",
    "- `S_Active_Listening`: 7 unique values (int64)\n",
    "\n",
    "## Nominal Variables (Categorical)\n",
    "- `Customer_id`: 10000 unique values (object)\n",
    "- `Interaction`: 10000 unique values (object)\n",
    "- `UID`: 10000 unique values (object)\n",
    "- `City`: 6072 unique values (object)\n",
    "- `State`: 52 unique values (object)\n",
    "- `County`: 1607 unique values (object)\n",
    "- `Zip`: 8612 unique values (int64)\n",
    "- `Area`: 3 unique values (object)\n",
    "- `TimeZone`: 26 unique values (object)\n",
    "- `Job`: 639 unique values (object)\n",
    "- `Marital`: 5 unique values (object)\n",
    "- `Gender`: 3 unique values (object)\n",
    "- `ReAdmis`: 2 unique values (object)\n",
    "- `Soft_drink`: 2 unique values (object)\n",
    "- `Initial_admin`: 3 unique values (object)\n",
    "- `HighBlood`: 2 unique values (object)\n",
    "- `Stroke`: 2 unique values (object)\n",
    "- `Complication_risk`: 3 unique values (object)\n",
    "- `Overweight`: 2 unique values (object)\n",
    "- `Arthritis`: 2 unique values (object)\n",
    "- `Diabetes`: 2 unique values (object)\n",
    "- `Hyperlipidemia`: 2 unique values (object)\n",
    "- `BackPain`: 2 unique values (object)\n",
    "- `Anxiety`: 2 unique values (object)\n",
    "- `Allergic_rhinitis`: 2 unique values (object)\n",
    "- `Reflux_esophagitis`: 2 unique values (object)\n",
    "- `Asthma`: 2 unique values (object)\n",
    "- `Services`: 4 unique values (object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given the nature of the data, there are several variables that will be excluded from the analysis. Here is a brief summary of the variables that will be excluded and the rationale for their exclusion:**\n",
    "\n",
    "### Current Strategy Overview:\n",
    "1. **Broad Inclusion**: Start with a wide array of variables to capture potential influences on `Initial_days`.\n",
    "2. **Build Initial Model**: Use this extensive dataset to identify significant predictors.\n",
    "3. **Analyze & Refine**: Eliminate non-contributing or highly correlated variables based on initial model insights.\n",
    "4. **Develop Reduced Model**: Focus on key variables for a streamlined, effective model.\n",
    "\n",
    "### Variables Eliminated:\n",
    "- **TotalCharge & Additional Charges**: Possible high correlation and generally a result of `Initial_days` not a cause of.\n",
    "- **Latitude & Longitude**: Limited interpretive value.\n",
    "- **Identifiers (Customer_id, Interaction, UID)**: High uniqueness; ethical concerns.\n",
    "- **Geographic (City, State, County, Zip, Population)**: Overly detailed, increasing model complexity.\n",
    "- **TimeZone**: Relevance to hospital stay length is questionable.\n",
    "- **Job**: Subjective and variable in interpretation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create reduced dataframe with only the columns  for the analysis\n",
    "# columns to drop\n",
    "colms_to_drop = ['TotalCharge', 'Additional_charges', 'Lat', 'Lng', 'Customer_id', 'Interaction', 'UID', 'City', 'State', 'County', 'Zip', 'TimeZone', 'Job', 'Population']\n",
    "\n",
    "# Creates list of columns except the 'Initial_days'\n",
    "remaining_cols = [col for col in df_medical.columns if col not in colms_to_drop + ['Initial_days']]\n",
    "\n",
    "# Addd 'Initial_days' to the end of column list - for backwards elimination intuition\n",
    "final_cols = remaining_cols + ['Initial_days']\n",
    "\n",
    "# crerate final df with the proper column order\n",
    "df_reduced = df_medical[final_cols].copy()\n",
    "\n",
    "# display the dataframe\n",
    "df_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Stats For ratio and interval variables (numerical summary)\n",
    "selected_columns = df_reduced[['Age', 'Income', 'VitD_levels', 'Doc_visits', 'Full_meals_eaten', 'vitD_supp', 'Initial_days']].copy()\n",
    "selected_columns.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Takeaways:\n",
    "\n",
    "- **Age**: Averages 53 years, ranging from 18 to 89, with a diverse age profile.\n",
    "- **Income**: Averages $40,490, with wide variation (154 to 207249), indicating economic diversity.\n",
    "- **VitD_levels**: Averages 17.96, mostly within a narrow range (9.81 to 26.39), suggesting more consistent levels across patients.\n",
    "- **Doc_visits**: Averages 5 visits, indicating a similar frequency of medical consultations.\n",
    "- **Full_meals_eaten**: Averages slightly over 1 meal a day, with most patients having similar meal frequencies.\n",
    "- **vitD_supp**: Averages less than 0.5 supplements, with low intake common among patients.\n",
    "\n",
    "- **Categorical** nominal and ordinal variables are not included here and will include a separate summary of proportions along wit univariate and bivariate visualizations.\n",
    "- **Initial_days**: Our dependent (target) variable will be fully summarize and visualized below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv and then read back to to save results so far and to reduce memory consumption.\n",
    "df_reduced.to_csv('df_reduced.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('df_reduced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot for 'Initial_days'\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(x=df['Initial_days'])\n",
    "plt.title('Boxplot for Initial_days')\n",
    "plt.show()\n",
    "\n",
    "# Histogram for 'Initial_days'\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.histplot(data=df, x='Initial_days', kde=True)\n",
    "plt.title('Histogram for Initial_days')\n",
    "plt.show()\n",
    "\n",
    "df['Initial_days'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Boxplot Observations**: The median appears to be above the mid-30s, suggesting that roughly half of the patients have shorter initial stays and the other half have longer. There are no visible outliers, indicating no extreme values or anomalies that fall outside the typical range. The interfertile range shows that the middle 50% of the data spans a rather large range, suggesting a concentration of data within this segment.\n",
    "\n",
    "- **Histogram Observations**: The distribution is bimodal, with two peaks: one just under a few days and another around 70 days. This suggests there are two groups of patients with different typical hospital stay lengths. The histogram indicates that shorter initial stays are more common than longer stays, with a significant drop-off in frequency as the number of days increases towards the middle values. The spread between the two modes shows that there is variability in the data, not concentrated around a single central value. The bimodal distribution could imply two prevalent groups or clusters within the dataset for `Initial_days`. Understanding the reasons behind this bimodal distribution may require further investigation into the underlying factors affecting hospital stay lengths. This distribution is important to kee in mind when interpreting the results of the regression analysis, as it may influence the model's predictive accuracy and the significance of the predictors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**: Statistical measures for `Initial_days` across all patients in the dataset, including:\n",
    "\n",
    "- **Count**: 10,000 observations. This represents the number of patients included in the analysis.\n",
    "- **Mean**: Approximately 34 days. On average, patients spend a little over a month in the hospital.\n",
    "- **Standard Deviation**: About 26 days. This indicates a wide variation in the length of hospital stays among patients; while some patients have short stays, others have significantly longer stays.\n",
    "- **Minimum**: Just over 1 day. This shows that some patients are discharged almost immediately after admission.\n",
    "- **25% (First Quartile)**: About 8 days or less. A quarter of the patients have hospital stays just over a week.\n",
    "- **Median (50%)**: Approximately 36 days. This is very close to the mean. However, the slight difference between the mean and median indicates a slight skew in the data.\n",
    "- **75% (Third Quartile)**: About 61 days or less. Most patients are discharged within two months.\n",
    "- **Maximum**: Nearly 72 days. Indicates that some patients have extended hospital stays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C3.  Visualizations \n",
    "\n",
    "Univariate and Bivariate Visualizations for independent variables and their relationship with the dependent variable `Initial_days`. Seaborn and Matplotlib will be used to create visualizations and the choice of graph will depend on the nature of the variable being visualized. The web article by RealPython education site and Seaborn own documentation will be used as a guide for the visualizations. (sourcew) https://realpython.com/python-seaborn/ and (source) https://seaborn.pydata.org/introduction.html. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age\n",
    "plt.figure(figsize=(6, 4), facecolor='white')\n",
    "sns.histplot(data=df, x='Age', kde=True)\n",
    "plt.title('Distribution of Age')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Bivariate Age vs. Initial_days with best fit line\n",
    "plt.figure(figsize=(6, 4), facecolor='white')\n",
    "sns.regplot(data=df, x='Age', y='Initial_days', color='orange', scatter_kws={'edgecolor': 'black'})\n",
    "plt.title('Age vs. Initial Days')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Initial Days')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VitD_levels\n",
    "plt.figure(figsize=(6, 4), facecolor='white')\n",
    "sns.histplot(data=df_reduced, x='VitD_levels', kde=True)\n",
    "plt.title('Distribution of VitD_levels')\n",
    "plt.xlabel('VitD_levels')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Bivariate VitD_levels vs. Initial_days with best fit line\n",
    "plt.figure(figsize=(6, 4), facecolor='white')\n",
    "sns.regplot(data=df_reduced, x='VitD_levels', y='Initial_days', color='orange', scatter_kws={'edgecolor': 'black'})\n",
    "plt.title('VitD_levels vs. Initial Days')\n",
    "plt.xlabel('VitD_levels')\n",
    "plt.ylabel('Initial Days')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc_visits\n",
    "plt.figure(figsize=(6, 4), facecolor='white')\n",
    "sns.histplot(data=df_reduced, x='Doc_visits', kde=True)\n",
    "plt.title('Distribution of Doc_visits')\n",
    "plt.xlabel('Doc_visits')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Bivariate Doc_visits vs. Initial_days with best fit line\n",
    "plt.figure(figsize=(6, 4), facecolor='white')\n",
    "sns.regplot(data=df_reduced, x='Doc_visits', y='Initial_days', color='orange', scatter_kws={'edgecolor': 'black'})\n",
    "plt.title('Doc_visits vs. Initial Days')\n",
    "plt.xlabel('Doc_visits')\n",
    "plt.ylabel('Initial Days')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full_meals_eaten\n",
    "plt.figure(figsize=(6, 4), facecolor='white')\n",
    "sns.histplot(data=df_reduced, x='Full_meals_eaten', kde=True)\n",
    "plt.title('Distribution of Full_meals_eaten')\n",
    "plt.xlabel('Full_meals_eaten')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Bivariate Full_meals_eaten vs. Initial_days with best fit line\n",
    "plt.figure(figsize=(6, 4), facecolor='white')\n",
    "sns.regplot(data=df_reduced, x='Full_meals_eaten', y='Initial_days', color='orange', scatter_kws={'edgecolor': 'black'})\n",
    "plt.title('Full_meals_eaten vs. Initial Days')\n",
    "plt.xlabel('Full_meals_eaten')\n",
    "plt.ylabel('Initial Days')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vitD_supp\n",
    "plt.figure(figsize=(6, 4), facecolor='white')\n",
    "sns.histplot(data=df_reduced, x='vitD_supp', kde=True)\n",
    "plt.title('Distribution of vitD_supp')\n",
    "plt.xlabel('vitD_supp')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Bivariate vitD_supp vs. Initial_days with best fit line\n",
    "plt.figure(figsize=(6, 4), facecolor='white')\n",
    "sns.regplot(data=df_reduced, x='vitD_supp', y='Initial_days', color='orange', scatter_kws={'edgecolor': 'black'})\n",
    "plt.title('vitD_supp vs. Initial Days')\n",
    "plt.xlabel('vitD_supp')\n",
    "plt.ylabel('Initial Days')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discrete Variable Children\n",
    "plt.figure(figsize=(6, 4), facecolor='white')\n",
    "sns.countplot(data=df, x='Children')\n",
    "plt.title('Distribution of Children')\n",
    "plt.xlabel('Number of Children')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discrete Variable Children\n",
    "plt.figure(figsize=(6, 4), facecolor='white')\n",
    "sns.countplot(data=df, x='Children')\n",
    "plt.title('Distribution of Children')\n",
    "plt.xlabel('Number of Children')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nominal Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nominal Variable Marital\n",
    "plt.figure(figsize=(6, 4), facecolor='white')\n",
    "sns.countplot(data=df, x='Marital')\n",
    "plt.title('Marital Status Distribution')\n",
    "plt.xlabel('Marital Status')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C4 Data Wrangling\n",
    "## Reexpression of categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since the dataset contains several categorical variables, it is essential to re-express these variables in a numerical format to facilitate their inclusion in the regression analysis. This process is known as one-hot encoding, and it involves creating dummy variables for each category within a categorical variable. The Pandas library provides a convenient method for performing this transformation using the `pd.get_dummies()` function. This function creates a new binary column for each category within a categorical variable, with a value of 1 indicating the presence of that category and 0 indicating its absence. The original categorical variable is then dropped from the dataset to avoid multicollinearity issues in the regression model.\n",
    "\n",
    "Ordinal variables will be re-expressed as well using pythons `replace` method.\n",
    "Nominal variables will be re-expressed using the `pd.get_dummies()` method in pandas in a method called one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_vars = ['ReAdmis', 'Soft_drink', 'HighBlood', 'Stroke', 'Overweight', 'Arthritis', 'Diabetes', 'Hyperlipidemia', 'BackPain', 'Anxiety', 'Allergic_rhinitis', 'Reflux_esophagitis', 'Asthma']\n",
    "\n",
    "df[binary_vars] = df[binary_vars].replace({'Yes': 1, 'No': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check the unique values for the binary variables\n",
    "for col in binary_vars:\n",
    "    print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data dictionary states: \"The (survey) variables represent responses to an eight question survey asking customers to rate the importance of various factors/surfaces on a scale of 1 to 8 (1 most important, 8 least important)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data dictionary states: \"The (Survey) variables represent responses to an eight question survey asking customers to rate the importance of various factors/surfaces on a scale of 1 to 8 (1 most important, 8 least important)\" Generally 1 is low and 8 is high in terms of importance and having 1 as most important and 8 as least important is not intuitive. Therefore, we will reverse the scale of survey variables so that 1 is the lowest and 8 is the highest in terms of importance. This will make the interpretation more intuitive in the regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse the scale of survey variables so that 1 is the lowest and 8 is the highest in terms of importance. \n",
    "survey_vars = ['S_T_Admission', 'S_T_Treatment', 'S_T_Visits', 'S_Reliability', 'S_Options', 'S_Hours_Treatment', 'S_Staff', 'S_Active_Listening']\n",
    "\n",
    "\n",
    "df[survey_vars] = df[survey_vars].replace({1: 8, 2: 7, 3: 6, 4: 5, 5: 4, 6: 3, 7: 2, 8: 1})\n",
    "\n",
    "df[survey_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G & H: References\n",
    "\n",
    "- Western Governors University. (2023, December 21). D207 - Medical_clean Dataset. Retrieved from https://lrps.wgu.edu/provision/227079957\n",
    "\n",
    "- Western Governors University IT Department. (2023). R or Python? How to decide which programming language to learn. Retrieved from https://www.wgu.edu/online-it-degrees/programming-languages/r-or-python.html#\n",
    "\n",
    "- Datacamp. (2023, December 12). D207 - Exploratory Data Analysis. Retrieved from https://app.datacamp.com/learn/custom-tracks/custom-d207-exploratory-data-analysis \n",
    "\n",
    "- Sewell, Dr. (2023). WGU D207 Exploratory Data Analysis [Webinars]. WGU Webex. Accessed December, 2023. https://wgu.webex.com/webappng/sites/wgu/meeting/info/c4aca2eac546482880f1557c938abf40?siteurl=wgu&MTID=me73470c2eac9e863c6f47a3d5b6d2f26 \n",
    "\n",
    "- Seaborn Developers. (2023). seaborn.scatterplot â€” seaborn 0.11.2 documentation. Retrieved December 22, 2023, from https://seaborn.pydata.org/generated/seaborn.scatterplot.html\n",
    "\n",
    "OLD ABOVE _ DELETE?KEEP? as needed.\n",
    "\n",
    "- Statology. (n.d.). *The Five Assumptions of Multiple Linear Regression*. Statology. Retrieved March 10, 2024, from www.statology.org/multiple-linear-regression-assumptions/\n",
    "\n",
    "- Pennsylvania State University. (n.d.). *5.3 - The Multiple Linear Regression Model*. STAT 501. Retrieved March 10, 2024, from online.stat.psu.edu/stat501/lesson/5/5.3\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations\n",
    "\n",
    "Beware of the following with your regression analysis:\n",
    "\n",
    "Overfitting can occur due to limited data points.\n",
    "\n",
    "Multicollinearity occurs when high association (correlation) with other IVs.\n",
    "\n",
    "P-values can be unreliable and coefficients swing wildly\n",
    "\n",
    "Check for pairwise correlations and high VIF (> 10)\n",
    "\n",
    "Tune your model with as many variables as practical. Forward, backward, stepwise\n",
    "    regression based on AIC, BIC, etc.\n",
    "ppoint 5 https://westerngovernorsuniversity-my.sharepoint.com/:p:/g/personal/william_sewell_wgu_edu/ERPQ0YpiQktOl-7YyAVnfLMBR5qeBh2cSv61VaJqe_aHKg?e=FjPhPz\n",
    "\n",
    "# Errata n notes\n",
    "\n",
    "I'm wrapping up task 1, and my research question is 'what factors influence the total charge a patient receives'. Total charge has a bimodal distribution that I did a log transform on which helped tremendously. Regarding my final reduced model, the RSE is pretty good, both residual normality and homoscedasticity are mostly there. Both have slight variance from expectations around the tails. For fun I decided to re run my code but filtered my data for patients staying less than a month and it improved my RSE, normality and homoscedasticity. Should I change my research question or keep it broad and just explain the limitations of outlier patients?\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
