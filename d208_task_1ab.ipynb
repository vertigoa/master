{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Performance Assessment: D208 Predictive Modeling Task 1 - Multiple Linear Regression.\n",
    "\n",
    "## Michael Hindes\n",
    "Department of Information Technology, Western Governors University\n",
    "<br>D208: Predictive Modeling\n",
    "<br>Professor Dr. Straw\n",
    "<br>February 11, 2024\n",
    "\n",
    "\n",
    "# Part I: Research Question\n",
    "## Describe the purpose of this data analysis by doing the following::\n",
    "\n",
    "### **A1. Research Question:**\n",
    "**\"A1. Research Question:\n",
    "\"What factors contribute to the length of a patient's hospital stay?\"**\n",
    "\n",
    "This question aims to identify key variables within the dataset that influence `Initial_days`; The number of days the patient stayed in the hospital during the initial visit to the hospital. \n",
    "\n",
    "### **A2. Define the goals of the data analysis.**\n",
    "\n",
    "The project sets out to explore the relationship between response and predictor variables by developing a multiple regression model based on medical data. The research question focuses on identifying any factors that affect the length of a patient's hospital stay, exploring variables such as demographic details, medical history, financial factors, and services received. Using Python for analysis, supported by visual aids for clarity, the aim is to address real-world healthcare challenges through a muktiple linear regression model. Data cleaning is emphasized to ensure accuracy and reliability.The Python code for analysis, data cleaning, and preparation will be shared. The culmination of this project involves creating and evaluating a multiple linear regression model, discussing its significance both statistically and practically, highlighting limitations, and suggesting actionable steps for healthcare organizations to enhance planning, patient care, and operational efficiency.\n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "# Part II: Method Justification\n",
    "\n",
    "## B. Describe multiple linear regression methods by doing the following:\n",
    "\n",
    "### **B1. Summarize four assumptions of a multiple linear regression model:**\n",
    "\n",
    "In multiple linear regression analysis, four key assumptions are critical: linearity between variables, independence of observations, constant error variance (homoscedasticity), and normal distribution of error terms. Understanding and checking these assumptions is essential for the model's reliability and accuracy, providing a solid basis for predictive analytics.\n",
    "\n",
    "-   **Linearity** asserts that there is a straight-line relationship between each predictor (independent variable) and the response (dependent variable). This means that changes in a predictor variable are associated with proportional changes in the response variable.\n",
    "\n",
    "-   **Independence of Observations** indicates that the data points in the dataset do not influence each other. Each observation's response is determined by its predictor values, free from the effects of other observations in the dataset.\n",
    "\n",
    "-   **Homoscedasticity** refers to the requirement that the error terms (differences between observed and predicted values) maintain a consistent variance across all levels of the independent variables. This constant variance ensures that the model's accuracy does not depend on the value of the predictors.\n",
    "\n",
    "-    **Normality of Errors** involves the assumption that for any fixed value of an independent variable, the error terms are normally distributed. This normal distribution is central to conducting various statistical tests on the model's coefficients to determine their significance.\n",
    "\n",
    "(Statology, n.d.)\n",
    "(Pennsylvania State University, n.d.)\n",
    "\n",
    "### **B2. Describe two benefits of using Python for data analysis:**\n",
    "\n",
    "- **Rich Libraries:** While R was specifically designed with statistics and data analysis in mind, Python distinguishes itself with its comprehensive suite of libraries that cater to virtually every phase of the data analysis process. Libraries such as Pandas for data manipulation allow for efficient handling and transformation of data, NumPy for numerical computations supports complex mathematical operations with ease, and Matplotlib along with Seaborn for visualization enable the creation of insightful and high-quality graphs and charts. Moreover, Scikit-learn offers a robust platform for applying machine learning algorithms, streamlining the development of predictive models. These libraries not only facilitate a wide range of data analysis tasks but also ensure that analysts have the tools needed to tackle complex data challenges effectively.\n",
    "\n",
    "- **Versatility** Python's syntax is known for its intuitive and readable nature, making it an accessible choice for professionals across various domains, from data science to web development. This versatility extends Python's utility beyond data analysis to other applications such as web development, automation, and deep learning, through frameworks and libraries like Flask, Selenium, and TensorFlow respectively. For instance, an analyst can easily switch from analyzing data to deploying a machine-learning model as a web application within the same programming environment. This seamless integration across different tasks enables a smooth workflow and promotes a holistic approach to problem-solving in today's interconnected digital landscape.\n",
    "\n",
    "### **B3. Explain why multiple linear regression is an appropriate technique for analyzing the research question summarized in part I:**\n",
    "\n",
    "Multiple linear regression is particularly suited for addressing the research question at hand, as it facilitates the exploration of how several independent variables collectively influence a single continuous dependent variable, in this case, `Initial_days`. This analytical technique is adept at not only identifying but also quantifying the strength and nature of the relationships between Initial_days and various predictors, such as financial aspects, services rendered, and patient risk factors. By accounting for multiple factors simultaneously, multiple linear regression can provide nuanced insights into their combined effects on the length of a hospital stay. This comprehensive understanding is crucial for building robust predictive models that can inform decision-making processes.\n",
    "\n",
    "# NEEDS EDIT\n",
    " Part III: Data Preparation\n",
    "\n",
    "## C. Summarize the data preparation process for multiple linear regression analysis by doing the following:\n",
    "\n",
    "### *C1. Describe your data cleaning goals and the steps used to clean the data to achieve the goals that align with your research question including your annotated code.**\n",
    "\n",
    "*   **Importing the Data**: Use`pd.read_csv()` to import data into a Pandas DataFrame.\n",
    "    \n",
    "*   **Initial Data Examination**: Using `df.head()` provides a quick snapshot of the dataset, including a view of the first few rows. This helps in getting a preliminary understanding of the data's structure and content.\n",
    "    \n",
    "*   **Checking Data Types**: The `df.info()` method is used for assessing the dataset's overall structure, including the data types of each column and the presence of non-null values. \n",
    "    \n",
    "*   **Identifying Duplicate Rows**: Utilizing `df.duplicated()` to find duplicate rows is an essential cleaning step. Duplicates can skew your analysis and lead to inaccurate models. Once identified, you can decide whether to remove these rows with `df.drop_duplicates()` depending on their relevance to your research question.\n",
    "    \n",
    "*   **Detecting Missing Values**: The `df.isnull().sum()` command is instrumental in identifying missing values across the dataset. Understanding where and how much data is missing is critical for deciding on imputation methods or if certain rows/columns should be excluded from the analysis.\n",
    "\n",
    "*   **Outlier Management Strategy:**: In this dataset, outliers are important to detect and be aware of. Particularly when creating predictive regression models. For example outliers can have a detrimental effect on our regression assumption and the model itself(MIDDLETON VIDEO) If there are outliers present, make sure that they are real is important. However, in the context of medical data, outliers can have an extra level of nuance. This is because in medical data, outliers are often the very things that we are interested in. For example, a patient with a very high cholesterol level or a very low blood pressure. These values are not errors, but rather important indicators of health conditions.\n",
    "\n",
    "- Therefore, in this analysis, we will look for outliers and pay close attention to the details of the variables themselves. Outliers will be noted, but not necessarily treated unless they are obvious data entry errors or if they hinder our model.\n",
    "\n",
    "*    **Reviewing Unique Values**: Although `df.unique()` is used to explore unique values in a Series, for dataframes, you might consider `df.nunique()` to see the number of unique values in each column or use `df['column_name'].unique()` to check unique values in specific columns. This step is valuable for understanding the diversity of information within your dataset, particularly for categorical data.\n",
    "\n",
    "*   **Drop Unnecessary Columns**: Any columns that are not relevant to the research question or the predictive model will be dropped from the dataset. \n",
    "\n",
    "*   **Categorical variable conversion**: Categorical variables will be transformed into numerical formats. Demographic data, which represents static information about patients and cannot be altered by the hospital, will be excluded from the analysis. We will identify and address any missing data, ensuring its proper mitigation. Additionally, any duplicate records identified in the dataset will be eliminated.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and libraries\n",
    "%pip install scikit-learn\n",
    "%pip install Jinja2\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Display variable description and data types with examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data variable description and data types with examples.\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(filename='variable_description_208.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data and read it into a dataframe, setting the first column CaseOrder as the index\n",
    "\n",
    "df_medical = pd.read_csv('D208_templates/medical_clean.csv', index_col=0)\n",
    "\n",
    "# Display the first five rows of the data\n",
    "df_medical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the last 5 rows of the dataframe\n",
    "df_medical.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the DataFrame information\n",
    "df_medical.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows. \n",
    "print(df_medical.duplicated().value_counts())\n",
    "\n",
    "# Display the count of duplicate rows\n",
    "print('Total Duplicated Rows: ', df_medical.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "df_medical.isnull().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values \n",
    "missing_values = df_medical.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C2.  EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns Item 1 to Item 8 to the appropriate column names. The 'S_' modifier is used to indicate the column is a survey item.\n",
    "new_col_names={\n",
    "    'Item1':'S_T_Admission',\n",
    "    'Item2':'S_T_Treatment', \n",
    "    'Item3':'S_T_Visits', \n",
    "    'Item4':'S_Reliability', 'Item5':'S_Options', \n",
    "    'Item6':'S_Hours_Treatment', \n",
    "    'Item7':'S_Staff', \n",
    "    'Item8':'S_Active_Listening'}\n",
    "df_medical.rename(columns=new_col_names, inplace=True)\n",
    "df_medical.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the data types and unique values count into a DataFrame easy reference and comparison\n",
    "data_types = df_medical.dtypes\n",
    "\n",
    "unique_values = df_medical.nunique()\n",
    "\n",
    "comparison_df = pd.DataFrame({'Data Type': data_types, 'Unique Values': unique_values})\n",
    "\n",
    "comparison_df.sort_values(by='Unique Values', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cardinality and Data Type Summary of Variables\n",
    "\n",
    "## Ratio Variables (Continuous with an absolute zero)\n",
    "- `Income`: 9993 unique values (float64)\n",
    "- `VitD_levels`: 9976 unique values (float64)\n",
    "- `Initial_days`: 9997 unique values (float64)\n",
    "- `TotalCharge`: 9997 unique values (float64)\n",
    "- `Additional_charges`: 9418 unique values (float64)\n",
    "- `Population`: 5951 unique values (int64)\n",
    "- `Children`: 11 unique values (int64)\n",
    "- `Age`: 72 unique values (int64)\n",
    "- `Doc_visits`: 9 unique values (int64)\n",
    "- `Full_meals_eaten`: 8 unique values (int64)\n",
    "- `vitD_supp`: 6 unique values (int64)\n",
    "\n",
    "## Interval Variables (Continuous without an absolute zero)\n",
    "- `Lat`: 8588 unique values (float64)\n",
    "- `Lng`: 8725 unique values (float64)\n",
    "\n",
    "## Ordinal Variables\n",
    "- `S_T_Admission`: 8 unique values (int64)\n",
    "- `S_T_Treatment`: 7 unique values (int64)\n",
    "- `S_T_Visits`: 8 unique values (int64)\n",
    "- `S_Reliability`: 7 unique values (int64)\n",
    "- `S_Options`: 7 unique values (int64)\n",
    "- `S_Hours_Treatment`: 7 unique values (int64)\n",
    "- `S_Staff`: 7 unique values (int64)\n",
    "- `S_Active_Listening`: 7 unique values (int64)\n",
    "\n",
    "## Nominal Variables (Categorical)\n",
    "- `Customer_id`: 10000 unique values (object)\n",
    "- `Interaction`: 10000 unique values (object)\n",
    "- `UID`: 10000 unique values (object)\n",
    "- `City`: 6072 unique values (object)\n",
    "- `State`: 52 unique values (object)\n",
    "- `County`: 1607 unique values (object)\n",
    "- `Zip`: 8612 unique values (int64)\n",
    "- `Area`: 3 unique values (object)\n",
    "- `TimeZone`: 26 unique values (object)\n",
    "- `Job`: 639 unique values (object)\n",
    "- `Marital`: 5 unique values (object)\n",
    "- `Gender`: 3 unique values (object)\n",
    "- `ReAdmis`: 2 unique values (object)\n",
    "- `Soft_drink`: 2 unique values (object)\n",
    "- `Initial_admin`: 3 unique values (object)\n",
    "- `HighBlood`: 2 unique values (object)\n",
    "- `Stroke`: 2 unique values (object)\n",
    "- `Complication_risk`: 3 unique values (object)\n",
    "- `Overweight`: 2 unique values (object)\n",
    "- `Arthritis`: 2 unique values (object)\n",
    "- `Diabetes`: 2 unique values (object)\n",
    "- `Hyperlipidemia`: 2 unique values (object)\n",
    "- `BackPain`: 2 unique values (object)\n",
    "- `Anxiety`: 2 unique values (object)\n",
    "- `Allergic_rhinitis`: 2 unique values (object)\n",
    "- `Reflux_esophagitis`: 2 unique values (object)\n",
    "- `Asthma`: 2 unique values (object)\n",
    "- `Services`: 4 unique values (object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given the nature of the data, there are several variables that will be excluded from the analysis. Here is a brief summary of the variables that will be excluded and the rationale for their exclusion:**\n",
    "\n",
    "### Current Strategy Overview:\n",
    "1. **Broad Inclusion**: Start with a wide array of variables to capture potential influences on `Initial_days`, informed by my domain knowledge.\n",
    "2. **Build Initial Model**: Use this extensive dataset to identify significant predictors.\n",
    "3. **Analyze & Refine**: Eliminate non-contributing or highly correlated variables based on initial model insights.\n",
    "4. **Develop Reduced Model**: Focus on key variables for a streamlined, effective model.\n",
    "\n",
    "### Variables Eliminated:\n",
    "*Note: I am a former health care professional who has worked in several hospitals and have had extensive hospital stays as a patient and this domain knowledge informs my decision making here.*\n",
    "- **TotalCharge & Additional Charges**: Possible high correlation and generally a result of `Initial_days` not a cause of. Patients and staff often unaware of these charges until after the fact.\n",
    "- **Latitude & Longitude**: Limited interpretive value and adds to model complexity.\n",
    "- **Identifiers (Customer_id, Interaction, UID)**: High uniqueness; ethical concerns.\n",
    "- **Geographic (City, State, County, Zip, Population)**: Overly detailed, increasing model complexity, not short/medium term actionable.\n",
    "- **TimeZone**: Relevance to hospital stay length is questionable, increases complexity.\n",
    "- **Full_meals_eaten**: Restrictive and targeted diets and meals are so common and depends on patient and services that without context ths variable is not useful.\n",
    "- **Job**: Subjective and variable in interpretation. Better suited for targeted occupational study.\n",
    "- **Services**: All very common in diagnostic phase and itself dependent on too many unknown factors, and not likely to be significant predictors. Could add confusion. \n",
    "- **Soft_drink**: Poorly defined as soft drink can mean anything from uncaffinated carbonated water to a caffinated sugary soda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create reduced dataframe with only the columns  for the analysis\n",
    "# columns to drop\n",
    "colms_to_drop = ['TotalCharge', 'Services', 'Soft_drink', 'Additional_charges', 'Lat', 'Full_meals_eaten', 'Lng', 'Customer_id', 'Interaction', 'UID', 'City', 'State', 'County', 'Zip', 'TimeZone', 'Job', 'Population']\n",
    "\n",
    "# Creates list of columns except the 'Initial_days'\n",
    "remaining_cols = [col for col in df_medical.columns if col not in colms_to_drop + ['Initial_days']]\n",
    "\n",
    "# Addd 'Initial_days' to the end of column list - for backwards elimination intuition\n",
    "final_cols = remaining_cols + ['Initial_days']\n",
    "\n",
    "# crerate final df with the proper column order\n",
    "df_reduced = df_medical[final_cols].copy()\n",
    "\n",
    "# display the dataframe in full\n",
    "pd.set_option('display.max_columns', None)\n",
    "df_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Stats For numeric variables (numerical summary)\n",
    "selected_columns = df_reduced[['Age', 'Income', 'VitD_levels', 'Doc_visits', 'vitD_supp', 'Initial_days']].copy()\n",
    "selected_columns.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Takeaways:\n",
    "\n",
    "- **Age**: Averages 53 years, ranging from 18 to 89, with a diverse age profile.\n",
    "- **Income**: Averages $40,490, with wide variation (154 to 207249), indicating economic diversity.\n",
    "- **VitD_levels**: Averages 17.96, mostly within a narrow range (9.81 to 26.39), suggesting more consistent levels across patients.\n",
    "- **Doc_visits**: Averages 5 visits, indicating a similar frequency of medical consultations.\n",
    "- **vitD_supp**: Averages less than 0.5 supplements, with low intake common among patients.\n",
    "\n",
    "- **Categorical** nominal and ordinal variables are not included here and will include a separate summary of proportions along wit univariate and bivariate visualizations.\n",
    "- **Initial_days**: Our dependent (target) variable will be fully summarize and visualized below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rounding Justification. \n",
    "-    Rounding 'Initial_days' from 8 decimal places to 2significantly reduces the number of unique values, which can simplify analyses and visualizations by reducing the granularity of the data. Precision beyond 2 decimal places does not add meaningful information for the analysis. In many practical scenarios, especially related to days, a precision of 2 decimal places is sufficient to capture relevant variations without unnecessarily complicating the dataset.  In healthcare data, for instance, it's unlikely that fractions of a day to eight decimal places would impact decisions or care outcomes.\n",
    "\n",
    "- Similarly, rounding 'Income' to whole numbers, and 'VitD_levels' to 2 decimal places seems appropriate in this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round 'Initial_days' and 'VitD_levels' to 2 decimal places\n",
    "df_reduced = df_reduced.round({'VitD_levels': 2})\n",
    "df_reduced = df_reduced.round({'Initial_days': 2})\n",
    "\n",
    "# Round 'Income' to 0 decimal places by converting to integer\n",
    "df_reduced = df_reduced.astype({'Income': 'int64'})\n",
    "\n",
    "# Display the dataframe with the rounded values\n",
    "df_reduced[['Initial_days', 'VitD_levels', 'Income']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv and then read back to to save results so far and to reduce memory consumption.\n",
    "df_reduced.to_csv('df_reduced.csv', index='CaseOrder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('df_reduced.csv', index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot for 'Initial_days'\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(x=df['Initial_days'])\n",
    "plt.title('Boxplot for Initial_days')\n",
    "plt.show()\n",
    "\n",
    "# Histogram for 'Initial_days'\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.histplot(data=df, x='Initial_days', kde=True)\n",
    "plt.title('Histogram for Initial_days')\n",
    "plt.show()\n",
    "\n",
    "df['Initial_days'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Boxplot Observations**: The median appears to be above the mid-30s, suggesting that roughly half of the patients have shorter initial stays and the other half have longer. There are no visible outliers, indicating no extreme values or anomalies that fall outside the typical range. The interfertile range shows that the middle 50% of the data spans a rather large range, suggesting a concentration of data within this segment.\n",
    "\n",
    "- **Histogram Observations**: The distribution is bimodal, with two peaks: one just under a few days and another around 70 days. This suggests there are two groups of patients with different typical hospital stay lengths. The histogram indicates that shorter initial stays are more common than longer stays, with a significant drop-off in frequency as the number of days increases towards the middle values. The spread between the two modes shows that there is variability in the data, not concentrated around a single central value. The bimodal distribution could imply two prevalent groups or clusters within the dataset for `Initial_days`. Understanding the reasons behind this bimodal distribution may require further investigation into the underlying factors affecting hospital stay lengths. This distribution is important to kee in mind when interpreting the results of the regression analysis, as it may influence the model's predictive accuracy and the significance of the predictors. Bimodal distributions can be challenging for regression models to capture accurately, as they may require more complex modeling techniques to account for the distinct groups within the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**: Statistical measures for `Initial_days` across all patients in the dataset, including:\n",
    "\n",
    "- **Count**: 10,000 observations. This represents the number of patients included in the analysis.\n",
    "- **Mean**: Approximately 34 days. On average, patients spend a little over a month in the hospital.\n",
    "- **Standard Deviation**: About 26 days. This indicates a wide variation in the length of hospital stays among patients; while some patients have short stays, others have significantly longer stays.\n",
    "- **Minimum**: Just over 1 day. This shows that some patients are discharged almost immediately after admission.\n",
    "- **25% (First Quartile)**: About 8 days or less. A quarter of the patients have hospital stays just over a week.\n",
    "- **Median (50%)**: Approximately 36 days. This is very close to the mean. However, the slight difference between the mean and median indicates a slight skew in the data.\n",
    "- **75% (Third Quartile)**: About 61 days or less. Most patients are discharged within two months.\n",
    "- **Maximum**: Nearly 72 days. Indicates that some patients have extended hospital stays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C3.  Visualizations \n",
    "\n",
    "Univariate and Bivariate Visualizations for independent variables and their relationship with the dependent variable `Initial_days`. Seaborn and Matplotlib will be used to create visualizations and the choice of graph will depend on the nature of the variable being visualized. The web article by RealPython education site and Seaborn own documentation will be used as a guide for the visualizations. (sourcew) https://realpython.com/python-seaborn/ and (source) https://seaborn.pydata.org/introduction.html. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univaraite Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subplots for the histplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
    "\n",
    "# distribution/count of children\n",
    "sns.histplot(data=df, x='Children', ax=axes[0], kde=True)\n",
    "axes[0].set_title('Distribution/Count of Children')\n",
    "\n",
    "# distribution of ages\n",
    "sns.histplot(data=df, x='Age', ax=axes[1], kde=True)\n",
    "axes[1].set_title('Distribution of Ages')\n",
    "\n",
    "# distribution of income\n",
    "sns.histplot(data=df, x='Income', ax=axes[2], kde=True)\n",
    "axes[2].set_title('Distribution of Income')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# summary statistics for the variables\n",
    "df[['Children', 'Age', 'Income']].describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subplots for the boxplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
    "\n",
    "# boxplot of children\n",
    "sns.boxplot(data=df, x='Children', ax=axes[0])\n",
    "axes[0].set_title('Boxplot of Children')\n",
    "\n",
    "# boxplot of ages\n",
    "sns.boxplot(data=df, x='Age', ax=axes[1])\n",
    "axes[1].set_title('Boxplot of Ages')\n",
    "\n",
    "# boxplot of income\n",
    "sns.boxplot(data=df, x='Income', ax=axes[2])\n",
    "axes[2].set_title('Boxplot of Income')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The outliers here will be noted as they may impact the regression model, particularly with OLS regression. For now, we will note them and include as is in the initial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subplots for the histplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
    "\n",
    "# distribution/count of VitD_levels\n",
    "sns.histplot(data=df, x='VitD_levels', ax=axes[0], kde=True)\n",
    "axes[0].set_title('Distribution of VitD_levels')\n",
    "\n",
    "# distribution/count of Doc_visits with bigger bins\n",
    "sns.histplot(data=df, x='Doc_visits', ax=axes[1], kde=True)\n",
    "axes[1].set_title('Distribution/Count of Doc_visits')\n",
    "\n",
    "# distribution/count of vitD_supp with bigger bins\n",
    "sns.histplot(data=df, x='vitD_supp', ax=axes[2], kde=True)\n",
    "axes[2].set_title('Distribution/count of vitD_supp')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# descriptive statistics for the variables\n",
    "df[['VitD_levels', 'Doc_visits', 'vitD_supp']].describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `Vitamin D levels` appear normally distributed around a middle value, suggesting that most patients have Vitamin D levels within a standard range, with fewer individuals having very high or very low levels. `Doc_visits` show a pattern with most patientss having 4-6 visits, and the frequency drops for higher numbers of visits. For Vitamin `D supplements`, most patients are not given supplements, which aligns with the distribution of Vitamin D levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2 by 2 subplot grid\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 6))\n",
    "\n",
    "# Area\n",
    "sns.countplot(data=df, x='Area', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Count of Area')\n",
    "\n",
    "# Marital\n",
    "sns.countplot(data=df, x='Marital', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Count of Marital Status')\n",
    "\n",
    "# Gender\n",
    "sns.countplot(data=df, x='Gender', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Count of Gender')\n",
    "\n",
    "# ReAdmis\n",
    "sns.countplot(data=df, x='ReAdmis', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Numer of ReAdmissions')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# create a countplot for initial_admin\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.countplot(data=df, x='Initial_admin')\n",
    "plt.title('Count of Initial_admin')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proportion Summary \n",
    "\n",
    "`Area`\n",
    "- Rural: 33.69%\n",
    "- Urban: 33.03%\n",
    "- Suburban: 33.28%\n",
    "\n",
    "`Gender`\n",
    "- Female: 50.18%\n",
    "- Male: 47.68%\n",
    "- Nonbinary: 2.14%\n",
    "\n",
    "`Marital`\n",
    "- Widowed: 20.45% \n",
    "- Married: 20.23% \n",
    "- Separated: 19.87% \n",
    "- Never Married: 19.84% \n",
    "- Divorced: 19.61%\n",
    "\n",
    "`ReAdmis`\n",
    "- No: 63.31%\n",
    "- Yes: 36.69%\n",
    "\n",
    "`Initial_admin`\n",
    "- Emergency: 51.60%\n",
    "- Elective: 25.04%\n",
    "- Observation: 24.36%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2 by 4 subplot grid\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "\n",
    "# S_T_Admission\n",
    "sns.countplot(data=df, x='S_T_Admission', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Count of S_T_Admission')\n",
    "\n",
    "# S_T_Treatment\n",
    "sns.countplot(data=df, x='S_T_Treatment', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Count of S_T_Treatment')\n",
    "\n",
    "# S_T_Visits\n",
    "sns.countplot(data=df, x='S_T_Visits', ax=axes[0, 2])\n",
    "axes[0, 2].set_title('Count of S_T_Visits')\n",
    "\n",
    "# S_Reliability\n",
    "sns.countplot(data=df, x='S_Reliability', ax=axes[0, 3])\n",
    "axes[0, 3].set_title('Count of S_Reliability')\n",
    "\n",
    "# S_Options\n",
    "sns.countplot(data=df, x='S_Options', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Count of S_Options')\n",
    "\n",
    "# S_Hours_Treatment\n",
    "sns.countplot(data=df, x='S_Hours_Treatment', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Count of S_Hours_Treatment')\n",
    "\n",
    "# S_Staff\n",
    "sns.countplot(data=df, x='S_Staff', ax=axes[1, 2])\n",
    "axes[1, 2].set_title('Count of S_Staff')\n",
    "\n",
    "# S_Active_Listening\n",
    "sns.countplot(data=df, x='S_Active_Listening', ax=axes[1, 3])\n",
    "axes[1, 3].set_title('Count of S_Active_Listening')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# value counts for the survey items\n",
    "df[['S_T_Admission', 'S_T_Treatment', 'S_T_Visits', 'S_Reliability', 'S_Options', 'S_Hours_Treatment', 'S_Staff', 'S_Active_Listening']].apply(pd.Series.value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Survey responses across various rating scales appear to be fairly evenly distributed among the different survey items. This uniformity could indicate a degree of correlation among the responses to these items. To explore potential patterns, we will utilize pie charts to visualize the distribution of responses and a correlation matrix to quantitatively assess the relationships between the items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textprops = {\"fontsize\":10}  # Change the font size here\n",
    "\n",
    "# 2 by 4 subplot grid\n",
    "fig, axes = plt.subplots(4, 2, figsize=(6, 10))\n",
    "\n",
    "# S_T_Admission\n",
    "df['S_T_Admission'].value_counts().plot.pie(ax=axes[0, 0], autopct='%1.1f%%', textprops=textprops)\n",
    "axes[0, 0].set_title('S_T_Admission')\n",
    "\n",
    "# S_T_Treatment\n",
    "df['S_T_Treatment'].value_counts().plot.pie(ax=axes[0, 1], autopct='%1.1f%%', textprops=textprops)\n",
    "axes[0, 1].set_title('S_T_Treatment')\n",
    "\n",
    "# S_T_Visits\n",
    "df['S_T_Visits'].value_counts().plot.pie(ax=axes[1, 0], autopct='%1.1f%%', textprops=textprops)\n",
    "axes[1, 0].set_title('S_T_Visits')\n",
    "\n",
    "# S_Reliability\n",
    "df['S_Reliability'].value_counts().plot.pie(ax=axes[1, 1], autopct='%1.1f%%', textprops=textprops)\n",
    "axes[1, 1].set_title('S_Reliability')\n",
    "\n",
    "# S_Options\n",
    "df['S_Options'].value_counts().plot.pie(ax=axes[2, 0], autopct='%1.1f%%', textprops=textprops)\n",
    "axes[2, 0].set_title('S_Options')\n",
    "\n",
    "# S_Hours_Treatment\n",
    "df['S_Hours_Treatment'].value_counts().plot.pie(ax=axes[2, 1], autopct='%1.1f%%', textprops=textprops)\n",
    "axes[2, 1].set_title('S_Hours_Treatment')\n",
    "\n",
    "# S_Staff\n",
    "df['S_Staff'].value_counts().plot.pie(ax=axes[3, 0], autopct='%1.1f%%', textprops=textprops)\n",
    "axes[3, 0].set_title('S_Staff')\n",
    "\n",
    "# S_Active_Listening\n",
    "df['S_Active_Listening'].value_counts().plot.pie(ax=axes[3, 1], textprops=textprops)\n",
    "axes[3, 1].set_title('S_Active_Listening')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# display descriptive statistics for the survey items\n",
    "df[['S_T_Admission', 'S_T_Treatment', 'S_T_Visits', 'S_Reliability', 'S_Options', 'S_Hours_Treatment', 'S_Staff', 'S_Active_Listening']].describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The pie charts and summary show similar pattern across with some survey responses having almost identical proportions. This could indicate a lack of variability in the responses, which may impact the predictive power of these variables in the regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a correlation matrix\n",
    "#  the columns correlation matrix\n",
    "cols = ['S_T_Admission', 'S_T_Treatment', 'S_T_Visits', 'S_Reliability',\n",
    "        'S_Options', 'S_Hours_Treatment', 'S_Staff', 'S_Active_Listening']\n",
    "\n",
    "corr_matrix = df[cols].corr()\n",
    "\n",
    "# Create a heatmap of the correlation matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The correlation matrix shows that there may indeed be correlation amongst the different survey items. This could introduce multicolinarity into the regression model.\n",
    "\n",
    "- These pairs of items have correlation coefficients very close to zero, suggesting that there is little to no linear relationship between them. When selecting variables for a regression model, these items might be preferred as they are less likely to introduce multicollinearity issues. We will note this during the initial model building phase. And check the VIF scores to confirm.\n",
    "\n",
    "- S_T_Admission and S_Reliability: -0.0046\n",
    "- S_T_Visits and S_Reliability: -0.0063\n",
    "- S_T_Admission and S_Options: -0.0037\n",
    "- S_T_Treatment and S_Options: -0.01\n",
    "- S_T_Visits and S_Options: -0.01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bivariate Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate Graphs with Initial_days\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# VitD_levels: Scatter plot of VitD_levels vs. Initial_days\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.regplot(data=df, x='VitD_levels', y='Initial_days', scatter_kws={'edgecolor':'black'}, line_kws={'color':'orange'})\n",
    "plt.title('VitD_levels vs. Initial_days')\n",
    "\n",
    "# Children: Scatter plot of Children vs. Initial_days\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.regplot(data=df, x='Children', y='Initial_days', scatter_kws={'edgecolor':'black'}, line_kws={'color':'orange'})\n",
    "plt.title('Children vs. Initial_days')\n",
    "\n",
    "# Age: Scatter plot of Age vs. Initial_days\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.regplot(data=df, x='Age', y='Initial_days', scatter_kws={'edgecolor':'black'}, line_kws={'color':'orange'})\n",
    "plt.title('Age vs. Initial_days')\n",
    "\n",
    "# Income: Scatter plot of Income vs. Initial_days\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.regplot(data=df, x='Income', y='Initial_days', scatter_kws={'edgecolor':'black'}, line_kws={'color':'orange'})\n",
    "plt.title('Income vs. Initial_days')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.regplot(data=df, x='Doc_visits', y='Initial_days', scatter_kws={'edgecolor':'black'}, line_kws={'color':'orange'})\n",
    "plt.title('Initial_days vs. Doc_visits')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - `Vitamin D` levels and initial days don't seem to have a clear pattern, with no obvious relationship. When it comes to `children` ther is no distinct trend, suggesting the number of children doesn't linearly affect the length of hospital stay. `Age` shows a spread of data across the age range without a strong trend. For `income`, there's more variability at higher income levels, but there is no clear pattern suggesting a strong relationship. Overall, these plots suggest that individually, these variables do not have a simple linear relationship with the number of initial days spent in the hospital. However, together they might. Income might benifit from a transformation to better understand the relationship. `Doc_visits` suggest that there is no strong, straightforward relationship between the number of doctor visits and the average initial days, as increased doctor visits do not correlate with either a significant increase or decrease in the initial days.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate Graphs with Initial_days\n",
    "plt.figure(figsize=(8, 8))\n",
    "# Marital: box plot showing distribution of Initial_days across Marital statuses\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.boxplot(data=df, x='Marital', y='Initial_days', color='lightgreen')\n",
    "plt.title('Initial_days across Marital statuses')\n",
    "\n",
    "# Gender: box plot showing distribution of Initial_days across Gender categories\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.boxplot(data=df, x='Gender', y='Initial_days', color='lightgreen')\n",
    "plt.title('Initial_days across Gender categories')\n",
    "\n",
    "# ReAdmis: box plot showing distribution of Initial_days for Readmission status\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.boxplot(data=df, x='ReAdmis', y='Initial_days', color='lightgreen')\n",
    "plt.title('Initial_days for Readmission status')\n",
    "\n",
    "# Area: box plot showing distribution of Initial_days across Area categories\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.boxplot(data=df, x='Area', y='Initial_days', color='lightgreen')\n",
    "plt.title('Initial_days across Area')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# boxplot with Initial_admin and Initial_days\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.boxplot(data=df, x='Initial_admin', y='Initial_days', color='lightgreen')\n",
    "plt.title('Boxplot for Initial_admin and Initial_days')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Marital` statuses plot shows that seperated and single patients tend to have the highest number of days in the hospital, and that divorced and married tended to spend fewer days. The `Gender` categories show slightly higher median Initial_days for males and non binary patients and a notably lower median for females compared to males. The `Readmission` plot is very interesting. It shows a significantly higher median and and grouping of `Initial_days` for readmitted patients compared to non-readmitted patients, with several outliers representing long stays among readmitted patients. Lastly, the Area plot demonstrates a slightly lower median for urban areas, suggesting hospital time is less for urban patients compared to rural and suburban patients. Interstingly, `Initial_admin` shows a higher median for elective admissions compared to emergency admissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 by 2 subplot grid\n",
    "fig, axes = plt.subplots(4, 2, figsize=(8, 16))\n",
    "\n",
    "# S_T_Admission\n",
    "sns.boxplot(data=df, x='S_T_Admission', y='Initial_days', ax=axes[0, 0], color='lightblue')\n",
    "axes[0, 0].set_title('S_T_Admission vs. Initial_days')\n",
    "\n",
    "# S_T_Treatment\n",
    "sns.boxplot(data=df, x='S_T_Treatment', y='Initial_days', ax=axes[0, 1], color='lightblue')\n",
    "axes[0, 1].set_title('S_T_Treatment vs. Initial_days')\n",
    "\n",
    "# S_T_Visits\n",
    "sns.boxplot(data=df, x='S_T_Visits', y='Initial_days', ax=axes[1, 0], color='lightblue')\n",
    "axes[1, 0].set_title('S_T_Visits vs. Initial_days')\n",
    "\n",
    "# S_Reliability\n",
    "sns.boxplot(data=df, x='S_Reliability', y='Initial_days', ax=axes[1, 1], color='lightblue')\n",
    "axes[1, 1].set_title('S_Reliability vs. Initial_days')\n",
    "\n",
    "# S_Options\n",
    "sns.boxplot(data=df, x='S_Options', y='Initial_days', ax=axes[2, 0], color='lightblue')\n",
    "axes[2, 0].set_title('S_Options vs. Initial_days')\n",
    "\n",
    "# S_Hours_Treatment\n",
    "sns.boxplot(data=df, x='S_Hours_Treatment', y='Initial_days', ax=axes[2, 1], color='lightblue')\n",
    "axes[2, 1].set_title('S_Hours_Treatment vs. Initial_days')\n",
    "\n",
    "# S_Staff\n",
    "sns.boxplot(data=df, x='S_Staff', y='Initial_days', ax=axes[3, 0], color='lightblue')\n",
    "axes[3, 0].set_title('S_Staff vs. Initial_days')\n",
    "\n",
    "# S_Active_Listening\n",
    "sns.boxplot(data=df, x='S_Active_Listening', y='Initial_days', ax=axes[3, 1], color='lightblue')\n",
    "axes[3, 1].set_title('S_Active_Listening vs. Initial_days')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Seeing these survey results in a bivariate plot with Initial_days is interesting in the variation in the median Initial_days across the different survey responses. Admittedly, I am a little unsure about how to interpret this. The initial thinking is that there is some interesting insights to gleam from this. Perhaps this suggests that the survey responses may have some predictive power in determining the length of a patient's hospital stay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G & H: References\n",
    "\n",
    "- Western Governors University. (2023, December 21). D207 - Medical_clean Dataset. Retrieved from https://lrps.wgu.edu/provision/227079957\n",
    "\n",
    "- Western Governors University IT Department. (2023). R or Python? How to decide which programming language to learn. Retrieved from https://www.wgu.edu/online-it-degrees/programming-languages/r-or-python.html#\n",
    "\n",
    "- Datacamp. (2023, December 12). D207 - Exploratory Data Analysis. Retrieved from https://app.datacamp.com/learn/custom-tracks/custom-d207-exploratory-data-analysis \n",
    "\n",
    "- Sewell, Dr. (2023). WGU D207 Exploratory Data Analysis [Webinars]. WGU Webex. Accessed December, 2023. https://wgu.webex.com/webappng/sites/wgu/meeting/info/c4aca2eac546482880f1557c938abf40?siteurl=wgu&MTID=me73470c2eac9e863c6f47a3d5b6d2f26 \n",
    "\n",
    "- Seaborn Developers. (2023). seaborn.scatterplot — seaborn 0.11.2 documentation. Retrieved December 22, 2023, from https://seaborn.pydata.org/generated/seaborn.scatterplot.html\n",
    "\n",
    "OLD ABOVE _ DELETE?KEEP? as needed.\n",
    "\n",
    "- Statology. (n.d.). *The Five Assumptions of Multiple Linear Regression*. Statology. Retrieved March 10, 2024, from www.statology.org/multiple-linear-regression-assumptions/\n",
    "\n",
    "- Pennsylvania State University. (n.d.). *5.3 - The Multiple Linear Regression Model*. STAT 501. Retrieved March 10, 2024, from online.stat.psu.edu/stat501/lesson/5/5.3\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manage memory by using gc.collect() to clear memory\n",
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations\n",
    "\n",
    "Beware of the following with your regression analysis:\n",
    "\n",
    "Overfitting can occur due to limited data points.\n",
    "\n",
    "Multicollinearity occurs when high association (correlation) with other IVs.\n",
    "\n",
    "P-values can be unreliable and coefficients swing wildly\n",
    "\n",
    "Check for pairwise correlations and high VIF (> 10)\n",
    "\n",
    "Tune your model with as many variables as practical. Forward, backward, stepwise\n",
    "    regression based on AIC, BIC, etc.\n",
    "ppoint 5 https://westerngovernorsuniversity-my.sharepoint.com/:p:/g/personal/william_sewell_wgu_edu/ERPQ0YpiQktOl-7YyAVnfLMBR5qeBh2cSv61VaJqe_aHKg?e=FjPhPz\n",
    "\n",
    "# Errata n notes\n",
    "\n",
    "I'm wrapping up task 1, and my research question is 'what factors influence the total charge a patient receives'. Total charge has a bimodal distribution that I did a log transform on which helped tremendously. Regarding my final reduced model, the RSE is pretty good, both residual normality and homoscedasticity are mostly there. Both have slight variance from expectations around the tails. For fun I decided to re run my code but filtered my data for patients staying less than a month and it improved my RSE, normality and homoscedasticity. Should I change my research question or keep it broad and just explain the limitations of outlier patients?\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
